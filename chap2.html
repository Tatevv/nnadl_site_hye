<!DOCTYPE html>
<html lang="en">
<!-- Produced from a LaTeX source file.  Note that the production is done -->
<!-- by a very rough-and-ready (and buggy) script, so the HTML and other  -->
<!-- code is quite ugly!  Later versions should be better.                -->

<head>
  <meta charset="utf-8">
  <meta name="citation_title" content="Neural Networks and Deep Learning">
  <meta name="citation_author" content="Nielsen, Michael A.">
  <meta name="citation_publication_date" content="2015">
  <meta name="citation_fulltext_html_url" content="http://neuralnetworksanddeeplearning.com">
  <meta name="citation_publisher" content="Determination Press">
  <link rel="icon" href="nnadl_favicon.ICO" />
  <title>Neural networks and deep learning</title>
  <script src="assets/jquery.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$']]}, "HTML-CSS": {scale: 92}, TeX: { equationNumbers: { autoNumber: "AMS" }}});
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  <link href="assets/style.css" rel="stylesheet">
  <link href="assets/pygments.css" rel="stylesheet">
  <link rel="stylesheet" href="https://code.jquery.com/ui/1.11.2/themes/smoothness/jquery-ui.css">

  <style>
    /* Adapted from */

    /* https://groups.google.com/d/msg/mathjax-users/jqQxrmeG48o/oAaivLgLN90J, */

    /* by David Cervone */

    @font-face {
      font-family: 'MJX_Math';
      src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot');
      /* IE9 Compat Modes */
      src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot?iefix') format('eot'),
      url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'),
      url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype'),
      url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/svg/MathJax_Math-Italic.svg#MathJax_Math-Italic') format('svg');
    }

    @font-face {
      font-family: 'MJX_Main';
      src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot');
      /* IE9 Compat Modes */
      src: url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot?iefix') format('eot'),
      url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'),
      url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype'),
      url('https://cdn.mathjax.org/mathjax/latest/fonts/HTML-CSS/TeX/svg/MathJax_Main-Regular.svg#MathJax_Main-Regular') format('svg');
    }
  </style>

</head>

<body>

  <div class="nonumber_header">
    <h2><a href="index.html">Նեյրոնային ցանցեր և խորը ուսուցում</a></h2>
  </div>
  <div class="section">
    <div id="toc">
      <p class="toc_title">
        <a href="index.html">Նեյրոնային ցանցեր և խորը ուսուցում</a>
      </p>
      <p class="toc_not_mainchapter">
        <a href="about.html">Ինչի՞ մասին է գիրքը</a>
      </p>
      <p class="toc_not_mainchapter">
        <a href="exercises_and_problems.html">Խնդիրների և վարժությունների մասին</a>
      </p>
      <p class='toc_mainchapter'>
        <a id="toc_using_neural_nets_to_recognize_handwritten_digits_reveal" class="toc_reveal" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';"><img id="toc_img_using_neural_nets_to_recognize_handwritten_digits" src="images/arrow.png" width="15px"></a>
        <a href="chap1.html">Ձեռագիր թվանշանների ճանաչում՝ օգտագործելով նեյրոնային ցանցեր</a>
        <div id="toc_using_neural_nets_to_recognize_handwritten_digits" style="display: none;">
          <p class="toc_section">
            <ul>
              <a href="chap1.html#perceptrons">
                <li>Պերսեպտրոններ</li>
              </a>
              <a href="chap1.html#sigmoid_neurons">
                <li>Սիգմոիդ նեյրոններ</li>
              </a>
              <a href="chap1.html#the_architecture_of_neural_networks">
                <li>Նեյրոնային ցանցերի կառուցվածքը</li>
              </a>
              <a href="chap1.html#a_simple_network_to_classify_handwritten_digits">
                <li>Պարզ ցանց ձեռագիր թվանշանների ճանաչման համար</li>
              </a>
              <a href="chap1.html#learning_with_gradient_descent">
                <li>Ուսուցում գրադիենտային վայրէջքի միջոցով</li>
              </a>
              <a href="chap1.html#implementing_our_network_to_classify_digits">
                <li>Թվանշանները ճանաչող ցանցի իրականացումը</li>
              </a>
              <a href="chap1.html#toward_deep_learning">
                <li>Խորը ուսուցմանն ընդառաջ</li>
              </a>
            </ul>
          </p>
        </div>
        <script>
          $('#toc_using_neural_nets_to_recognize_handwritten_digits_reveal').click(function() {
            var src = $('#toc_img_using_neural_nets_to_recognize_handwritten_digits').attr('src');
            if (src == 'images/arrow.png') {
              $("#toc_img_using_neural_nets_to_recognize_handwritten_digits").attr('src', 'images/arrow_down.png');
            } else {
              $("#toc_img_using_neural_nets_to_recognize_handwritten_digits").attr('src', 'images/arrow.png');
            };
            $('#toc_using_neural_nets_to_recognize_handwritten_digits').toggle('fast', function() {});
          });
        </script>
        <p class='toc_mainchapter'>
          <a id="toc_how_the_backpropagation_algorithm_works_reveal" class="toc_reveal" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';"><img id="toc_img_how_the_backpropagation_algorithm_works" src="images/arrow.png" width="15px"></a>
          <a href="chap2.html">Ինչպե՞ս է աշխատում հետադարձ տարածումը</a>
          <div id="toc_how_the_backpropagation_algorithm_works" style="display: none;">
            <p class="toc_section">
              <ul>
                <a href="chap2.html#warm_up_a_fast_matrix-based_approach_to_computing_the_output
    _from_a_neural_network">
                  <li>Մարզանք. նեյրոնային ցանցի ելքային արժեքների հաշվման արագագործ, մատրիցային մոտեցում</li>
                </a>
                <a href="chap2.html#the_two_assumptions_we_need_about_the_cost_function">
                  <li>Երկու ենթադրություն գնային ֆունկցիայի վերաբերյալ</li>
                </a>
                <a href="chap2.html#the_hadamard_product_$s_\odot_t$">
                  <li>Հադամարի արտադրյալը՝ $s \odot t$</li>
                </a>
                <a href="chap2.html#the_four_fundamental_equations_behind_backpropagation">
                  <li>Հետադարձ տարածման հիմքում ընկած չորս հիմնական հավասարումները</li>
                </a>
                <a href="chap2.html#proof_of_the_four_fundamental_equations_(optional)">
                  <li>Չորս հիմնական հավասարումների ապացույցները (ընտրովի)</li>
                </a>
                <a href="chap2.html#the_backpropagation_algorithm">
                  <li>Հետադարձ տարածման ալգորիթմը</li>
                </a>
                <a href="chap2.html#the_code_for_backpropagation">
                  <li>Հետադարձ տարածման իրականացման կոդը</li>
                </a>
                <a href="chap2.html#in_what_sense_is_backpropagation_a_fast_algorithm">
                  <li>Ի՞նչ իմաստով է հետադարձ տարածումն արագագործ ալգորիթմ</li>
                </a>
                <a href="chap2.html#backpropagation_the_big_picture">
                  <li>Հետադարձ տարածում. ամբողջական պատկերը</li>
                </a>
              </ul>
            </p>
          </div>

          <script>
            $('#toc_how_the_backpropagation_algorithm_works_reveal').click(function() {
              var src = $('#toc_img_how_the_backpropagation_algorithm_works').attr('src');
              if (src == 'images/arrow.png') {
                $("#toc_img_how_the_backpropagation_algorithm_works").attr('src', 'images/arrow_down.png');
              } else {
                $("#toc_img_how_the_backpropagation_algorithm_works").attr('src', 'images/arrow.png');
              };
              $('#toc_how_the_backpropagation_algorithm_works').toggle('fast', function() {});
            });
          </script>
          <p class='toc_mainchapter'>
            <a id="toc_improving_the_way_neural_networks_learn_reveal" class="toc_reveal" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';"><img id="toc_img_improving_the_way_neural_networks_learn" src="images/arrow.png" width="15px"></a>
            <a href="chap3.html">Նեյրոնային ցանցերի ուսուցման բարելավումը</a>
            <div id="toc_improving_the_way_neural_networks_learn" style="display: none;">
              <p class="toc_section">
                <ul>
                  <a href="chap3.html#the_cross-entropy_cost_function">
                    <li>Գնային ֆունկցիան՝ միջէնտրոպիայով</li>
                  </a>
                  <a href="chap3.html#overfitting_and_regularization">
                    <li>Գերմարզում և ռեգուլյարացում</li>
                  </a>
                  <a href="chap3.html#weight_initialization">
                    <li>Կշիռների սկզբնարժեքավորումը</li>
                  </a>
                  <a href="chap3.html#handwriting_recognition_revisited_the_code">
                    <li>Ձեռագրերի ճամաչման կոդի վերանայում</li>
                  </a>
                  <a href="chap3.html#how_to_choose_a_neural_network's_hyper-parameters">
                    <li>Ինչպե՞ս ընտրել նեյրոնային ցանցերի հիպեր-պարամետրերը</li>
                  </a>
                  <a href="chap3.html#other_techniques">
                    <li>Այլ տեխնիկաներ</li>
                  </a>
                </ul>
              </p>
            </div>
            <script>
              $('#toc_improving_the_way_neural_networks_learn_reveal').click(function() {
                var src = $('#toc_img_improving_the_way_neural_networks_learn').attr('src');
                if (src == 'images/arrow.png') {
                  $("#toc_img_improving_the_way_neural_networks_learn").attr('src', 'images/arrow_down.png');
                } else {
                  $("#toc_img_improving_the_way_neural_networks_learn").attr('src', 'images/arrow.png');
                };
                $('#toc_improving_the_way_neural_networks_learn').toggle('fast', function() {});
              });
            </script>
            <p class='toc_mainchapter'>
              <a id="toc_a_visual_proof_that_neural_nets_can_compute_any_function_reveal" class="toc_reveal" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';"><img id="toc_img_a_visual_proof_that_neural_nets_can_compute_any_function" src="images/arrow.png" width="15px"></a>
              <a href="chap4.html">Տեսողական ապացույց այն մասին, որ նեյրոնային ֆունկցիաները կարող են մոտարկել կամայական ֆունկցիա</a>
              <div id="toc_a_visual_proof_that_neural_nets_can_compute_any_function" style="display: none;">
                <p class="toc_section">
                  <ul>
                    <a href="chap4.html#two_caveats">
                      <li>Երկու զգուշացում</li>
                    </a>
                    <a href="chap4.html#universality_with_one_input_and_one_output">
                      <li>Ունիվերսալություն մեկ մուտքով և մեկ ելքով</li>
                    </a>
                    <a href="chap4.html#many_input_variables">
                      <li>Մեկից ավել մուտքային փոփոխականներ</li>
                    </a>
                    <a href="chap4.html#extension_beyond_sigmoid_neurons">
                      <li>Ընդլայնումը Սիգմոիդ նեյրոններից դուրս </li>
                    </a>
                    <a href="chap4.html#fixing_up_the_step_functions">
                      <li>Քայլի ֆունկցիայի ուղղումը</li>
                    </a>
                    <a href="chap4.html#conclusion">
                      <li>Եզրակացություն</li>
                    </a>
                  </ul>
                </p>
              </div>
              <script>
                $('#toc_a_visual_proof_that_neural_nets_can_compute_any_function_reveal').click(function() {
                  var src = $('#toc_img_a_visual_proof_that_neural_nets_can_compute_any_function').attr('src');
                  if (src == 'images/arrow.png') {
                    $("#toc_img_a_visual_proof_that_neural_nets_can_compute_any_function").attr('src', 'images/arrow_down.png');
                  } else {
                    $("#toc_img_a_visual_proof_that_neural_nets_can_compute_any_function").attr('src', 'images/arrow.png');
                  };
                  $('#toc_a_visual_proof_that_neural_nets_can_compute_any_function').toggle('fast', function() {});
                });
              </script>
              <p class='toc_mainchapter'>
                <a id="toc_why_are_deep_neural_networks_hard_to_train_reveal" class="toc_reveal" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';"><img id="toc_img_why_are_deep_neural_networks_hard_to_train" src="images/arrow.png" width="15px"></a>
                <a href="chap5.html">Ինչու՞մն է կայանում նեյրոնային ցանցերի մարզման բարդությունը</a>
                <div id="toc_why_are_deep_neural_networks_hard_to_train" style="display: none;">
                  <p class="toc_section">
                    <ul>
                      <a href="chap5.html#the_vanishing_gradient_problem">
                        <li>Անհետացող գրադիենտի խնդիրը</li>
                      </a>
                      <a href="chap5.html#what's_causing_the_vanishing_gradient_problem_unstable_gradients_in_deep_neural_nets">
                        <li>Ի՞նչն է անհետացող գրադիենտի խնդրի պատճառը։ Խորը նեյրոնային ցանցերի անկայուն գրադիենտները</li>
                      </a>
                      <a href="chap5.html#unstable_gradients_in_more_complex_networks">
                        <li>Անկայուն գրադիենտներն ավելի կոմպլեքս ցանցերում</li>
                      </a>
                      <a href="chap5.html#other_obstacles_to_deep_learning">
                        <li>Այլ խոչընդոտներ խորը ուսուցման մեջ</li>
                      </a>
                    </ul>
                  </p>
                </div>
                <script>
                  $('#toc_why_are_deep_neural_networks_hard_to_train_reveal').click(function() {
                    var src = $('#toc_img_why_are_deep_neural_networks_hard_to_train').attr('src');
                    if (src == 'images/arrow.png') {
                      $("#toc_img_why_are_deep_neural_networks_hard_to_train").attr('src', 'images/arrow_down.png');
                    } else {
                      $("#toc_img_why_are_deep_neural_networks_hard_to_train").attr('src', 'images/arrow.png');
                    };
                    $('#toc_why_are_deep_neural_networks_hard_to_train').toggle('fast', function() {});
                  });
                </script>
                <p class='toc_mainchapter'>
                  <a id="toc_deep_learning_reveal" class="toc_reveal" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';"><img id="toc_img_deep_learning" src="images/arrow.png" width="15px"></a>
                  <a href="chap6.html">Խորը ուսուցում</a>
                  <div id="toc_deep_learning" style="display: none;">
                    <p class="toc_section">
                      <ul>
                        <a href="chap6.html#introducing_convolutional_networks">
                          <li>Փաթույթային ցանցեր</li>
                        </a>
                        <a href="chap6.html#convolutional_neural_networks_in_practice">
                          <li>Փաթույթային ցանցերը կիրառության մեջ</li>
                        </a>
                        <a href="chap6.html#the_code_for_our_convolutional_networks">
                          <li>Փաթույթային ցանցերի կոդը</li>
                        </a>
                        <a href="chap6.html#recent_progress_in_image_recognition">
                          <li>Առաջխաղացումները պատկերների ճանաչման ասպարեզում</li>
                        </a>
                        <a href="chap6.html#other_approaches_to_deep_neural_nets">
                          <li>Այլ մոտեցումներ խորը նեյրոնային ցանցերի համար</li>
                        </a>
                        <a href="chap6.html#on_the_future_of_neural_networks">
                          <li>Նեյրոնային ցանցերի ապագայի մասին</li>
                        </a>
                      </ul>
                    </p>
                  </div>
                  <script>
                    $('#toc_deep_learning_reveal').click(function() {
                      var src = $('#toc_img_deep_learning').attr('src');
                      if (src == 'images/arrow.png') {
                        $("#toc_img_deep_learning").attr('src', 'images/arrow_down.png');
                      } else {
                        $("#toc_img_deep_learning").attr('src', 'images/arrow.png');
                      };
                      $('#toc_deep_learning').toggle('fast', function() {});
                    });
                  </script>
                  <p class="toc_not_mainchapter">
                    <a href="sai.html">Հավելված: Արդյո՞ք գոյություն ունի ինտելեկտի <em>պարզ</em> ալգորիթմ</a>
                  </p>
                  <p class="toc_not_mainchapter">
                    <a href="acknowledgements.html">Երախտագիտություն</a>
                  </p>
                  <p class="toc_not_mainchapter"><a href="faq.html">Հաճախ տրվող հարցեր</a>
                  </p>

                  <!--
<hr>

<p class="sidebar"> If you benefit from the book, please make a small
donation.  I suggest $3, but you can choose the amount.</p>

<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick">
<input type="hidden" name="encrypted" value="-----BEGIN PKCS7-----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-----END PKCS7-----
">
<input type="image" src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif" border="0" name="submit" alt="PayPal - The safer, easier way to pay online!">
<img alt="" border="0" src="https://www.paypalobjects.com/en_US/i/scr/pixel.gif" width="1" height="1">
</form>

-->

                  <hr>
                  <span class="sidebar_title">Հովանավորներ</span>
                  <br/>
                  <a href='http://www.ersatz1.com/'><img src='assets/ersatz.png' width='140px' style="padding: 0px 0px 10px 8px; border-style: none;"></a>

                  <a href='http://gsquaredcapital.com/'><img src='assets/gsquared.png' width='150px' style="padding: 0px 0px 10px 10px; border-style: none;"></a>

                  <a href='http://www.tineye.com'><img src='assets/tineye.png' width='150px'
style="padding: 0px 0px 10px 8px; border-style: none;"></a>

                  <a href='http://www.visionsmarts.com'><img
src='assets/visionsmarts.png' width='160px' style="padding: 0px 0px
0px 0px; border-style: none;"></a> <br/>

                  <p class="sidebar">Շնորհակալություն եմ հայտնում բոլոր <a href="supporters.html">աջակցողներին</a>, ովքեր օգնել են գիրքն իրականություն դարձնել: Հատուկ շնորհակալություններ Պավել Դուդրենովին. Շնորհակալություն եմ հայտնում նաև նրանց, ովքեր ներդրում են ունեցել
                    <a href="bugfinder.html">Սխալների որոնման հուշատախտակում</a>. </p>

                  <hr>
                  <span class="sidebar_title">Ռեսուրսներ</span>

                  <p class="sidebar"><a href="https://twitter.com/michael_nielsen">Մայքլ Նիլսենը թվիթերում</a></p>

                  <p class="sidebar"><a href="faq.html">Գրքի մասին հաճախակի տրբող հարցեր</a></p>

                  <p class="sidebar">
                    <a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Կոդի պահոցը</a></p>

                  <p class="sidebar">
                    <a href="http://eepurl.com/0Xxjb">Մայքլ Նիլսենի նախագծերի հայտարարման էլ հասցեների ցուցակը</a>
                  </p>

                  <p class="sidebar"> <a href="http://www.deeplearningbook.org/">Խորը Ուսուցում</a>, գրքի հեղինակներ` Յան Գուդֆելլո, Յոշուա Բենջիո և Ահարոն Կուրվիլ</p>

                  <p class="sidebar"><a href="http://cognitivemedium.com">cognitivemedium.com</a></p>

                  <hr>
                  <a href="http://michaelnielsen.org"><img src="assets/Michael_Nielsen_Web_Small.jpg" width="160px" style="border-style: none;"/></a>

                  <p class="sidebar">
                    <a href="http://michaelnielsen.org">Մայքլ Նիլսեն</a>, Հունվար 2017
                  </p>
    </div>

    <p>
      <a href="chap1.html">Նախորդ գլխում</a> տեսանք, թե ինչպես նեյրոնային ցանցերը կարող են գրադիենտային վայրէջքի օգնությամբ սովորել իրենց կշիռներն ու շեղումները։ Սակայն մեր բացատրություններում բաց թողում կար, այն է՝ ինչպե՞ս հաշվել գնի ֆունկցիայի գրադիենտը։
      Դա էական բացթողում է։ Այս գլխում կդիտարկենք գրադիենտների հաշվման արագագործ ալգորիթմ, որի անունն է
      <em>հետադարձ տարածում (backpropagation)</em>։
    </p>

    <p>
      Հետադարձ տարածման ալգորիթմի նկարագրությունն առաջին անգամ հանդիպել է 1970-ականներին, սակայն իր կարևորությունն ամբողջապես չի գնահատվել մինչ
      <a href="http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf">1986 թվականի այս հոդվածը</a>՝ հեղինակված
      <a href="http://en.wikipedia.org/wiki/David_Rumelhart">Դավիթ Ռումելհարթի</a>,
      <a href="http://en.wikipedia.org/wiki/David_Rumelhart">Ջոֆֆրի Հինթոնի</a> և
      <a href="http://en.wikipedia.org/wiki/Ronald_J._Williams">Ռոնալդ Վիլյամսի</a> կոմից։ Այդ հոդվածը նկարագրում է մի քանի նեյրոնային ցանցեր, որտեղ հետադարձ տարածումն աշխատում է շատ ավելի արագ, քան ավելի վաղ ժամանակներում օգտագործվող ուսուցման մեթոդները,
      այդպիսով, հնարավոր է դարձնում նեյրոնային ցանցերի օգտագործումը նախկինում անլուծելի խնդիրների լուծման նպատակով Այսօր հետադարձ տարածումը նեյրոնային ցանցերով ուսուցման հիմնական շարժիչ ուժն է։
    </p>

    <p>
      Այս գլխում ավելի շատ են զետեղված մաթեմատիկական դուրսբերումներ, քան մնացած այլ գլուխներում։ Եթե դուք հրապուրված չեք մաթեմատիկայով, ապա կարող եք այս գլուխը բաց թողնել և հետադարձ տարածումն ընկալել որպես «սև արկղ», որի մանրամասները գիտակցաբար անտեսում եք։
      Սակայն ինչու՞ ժամանակ ծախսել և այդ մանրամասները սովորել։
    </p>

    <p>
      Պատճառն, իհարկե, խորությամբ հասկանալու ձգտումն է։ Հետադարձ տարածման հիմքում ընկած է $C$ գնի ֆունկցիայի՝ նեյրոնային ցանցերի $w$ կշիռների (կամ $b$ շեղումների) նկատմամբ մասնակի ածանցյալի $\partial C / \partial w$ արտահայտությունը։ Այն ցույց է տալիս, թե ինչ
      «արագությամբ» է գինը փոփոխվում՝ կախված կշիռների և շեղումների փոփոխությունից։ Այս արտահայտությունը փոքր-ինչ բարդ տեսք ունի, սակայն այն միաժամանակ շատ գեղեցիկ է, որի յուրաքանչյուր մասնիկն ունի բնական, ինտուիտիվ բացատրություն։ Եվ, այսպիսով, հետադարձ
      տարածումը միայն արագագործ ալգորիթմ չէ, որը «ստիպված» ենք սովորել։ Իրականում այն տալիս է խորը ներըմբռնում (insight) այն մասին, թե ինչպես է կշիռների և շեղումների փոփոխությունն ազդում ցանցի վարքագծի վրա։ Այդ իսկ պատճառով այս ալգորիթմն արժանի է մանրամասն
      ուսուցման։
    </p>

    <p>
      Այսպիսով, եթե ցանկություն ունեք թռուցիկ կարդալ կամ ուղղակի ցատկել հեջորդ գլխին, ապա դա նորմալ է։ Գրքի շարունակությունն այնպես է շարադրված, որ այն հասանելի է անգամ եթե դուք հետադարձ տարածումը դիտարկեք որպես սև արկղ։ Իհարկե, ավելի ուշ, գրքում կան կետեր,
      որոնք հղվում են այս գլխի արդյունքներին, սակայն այդ կետերում սպասվում է, որ դուք կկարողանաք հասկանալ հիմնական եզրահանգումները անգամ եթե չեք հետևել ամբողջական տրամաբանությանը։
    </p>
    <p>
      <h3>
    <a name="warm_up_a_fast_matrix-based_approach_to_computing_the_output_from_a_neural_network"></a>
    <a href="#warm_up_a_fast_matrix-based_approach_to_computing_the_output_from_a_neural_network">
      Մարզանք. նեյրոնային ցանցի ելքային արժեքների հաշվման արագագործ, մատրիցային մոտեցում
    </a>
  </h3>
    </p>

    <p>
      Մինչ հետադարձ տարածումը քննարկելը, որպես նախավարժանք դիտարկենք նեյրոնային ցանցի ելքային արժեքը հաշվելու արագագործ մատրիցային ալգորիթմ։ Իրականում մենք այդ ալգորիթմը համառոտ կերպով հանդիպել ենք
      <a href="chap1.html#implementing_our_network_to_classify_digits">նախորդ
  գլխի վերջնահատվածում</a>, սակայն այն քննարկեցինք արագորեն, հետևաբար կվերադառնանք և ավելի մանրամասն կդիտարկենք այս բաժնում։ Հատկապես, դա հնարավորություն կտա մեզ ծանոթանալ այն նշանակումներին, որոնք օգտագործվում են հետադարձ տարածման մեջ՝ արդեն հայտնի
      համատեքստում։
    </p>

    <p>
      Նախ ցանցի կշիռներին տանք այնպիսի նշանակումներ, որոնք կօգտագործենք այդ կշիռներին հստակորեն հղվելու նպատակով։ Նշանակենք $w^l_{jk}$-ով ցանցի $(l-1)^{\rm րդ}$ շերտի $k^{\rm րդ}$ նեյրոնը $l^{\rm րդ}$ շերտի $j^{\rm րդ}$ նեյրոնին միացնող կապի կշիռը։ Այսպիսով,
      օրինակ, ներքևի դիագրամում պատկերված կշիռը երկրորդ շերտի չորրորդ նեյրոնը միացնում է երրորդ շերտի երկրորդ նեյրոնի հետ.
      <center>
        <img src="images/tikz16.png" />
      </center>

      Այս նշանակումն առաջին հայացքից կարող է ծանրաբեռնված թվալ և դուք բավականին ջանք գործադրեք իրեն ընտելանալու, ավելի հեշտ ընկալելու և իր շուրջ տրամաբանելու համար։ Օրինակ $j$ և $k$ ինդեքսների դասավորվածությունը կարող է անբնական թվալ։ Հնարավոր, որ է մտածեք,
      թե ավելի խելամիտ է $j$-ով նշանակել մուտքային նեյրոնը և $k$-ով նշանակել ելքային նեյրոնը տրված կշռի դեպքում և ոչ հակառակը՝ այնպես ինչպես արդեն արված է։ Ստորև բացատրված է այդ նշանակման պատճառը։
    </p>

    <p>
      Նմանատիպ նշանակումներ են օգտագործվում նաև ցանցի շեղումների և ակտիվացիաների համար։ Եվ այսպես, $l^{\rm րդ}$ շերտի $j^{\rm րդ}$ նեյրոնին համապատասխանող շեղումը կնշանակենք $b^l_j$-ով։ Ստորև ներկայացված դիագրամը ցույց է տալիս, թե ինչպես օգտագործել այս նշանակումները.
      <center>
        <img src="images/tikz17.png" />
      </center>
      Ըստ այս նշանակումների, $l^{\rm րդ}$ շերտի $j^{\rm րդ}$ նեյրոնի $a^{l}_j$ ակտիվացիան $(l-1)^{\rm րդ}$ շերտի նեյրոնների ակտիվացիաների հետ կապված է հետևյալ հավասարմամբ (կարող եք համեմատել

      <span id="margin_788574725314_reveal" class="equation_link">(4)</span>
      <span id="margin_788574725314" class="marginequation" style="display: none;">
    <a href="chap1.html#eqtn4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)} \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_788574725314_reveal').click(function() {
          $('#margin_788574725314').toggle('slow', function() {});
        });
      </script>

      հավասարման և իր շուրջ ծավալված քննարկման հետ).

      <a class="displaced_anchor" name="eqtn23"></a>\begin{eqnarray} a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right), \tag{23}\end{eqnarray} որտեղ գումարն ըստ $(l-1)^{\rm րդ}$ շերտի $k$ նեյրոնների է։ Սահմանենք $w^l$
      <em>կշիռների մատրիցը (weight matrix)</em>, որի միջոցով արտագրենք արտահայտությունը մատրիցային տեսքով՝ յուրաքանչյուր $l$ շերտի համար։ $w^l$ կշիռների մատրիցը կազմված է $l^{\rm րդ}$ շերտին կապվող նեյրոնների կշիռներից, այսինքն, $j^{\rm րդ}$ տողի $k^{\rm
      րդ}$ սյունակի էլեմենտը $w^l_{jk}$ կշիռն է։ Նույն ձևով, յուրաքանչյուր $l$ շերտի համար սահմանենք $b^l$ <em>շեղման վեկտոր (bias vector)</em>։ Ակնհայտ է, որ շեղման վեկտորի անդամները պարզապես $b^l_j$ արժեքներն են՝ $l^{\rm րդ}$ շերտի նեյրոնների շեղումները։
      Եվ վերջապես, սահմանենք $a^l$ ակտիվացիայի վեկտորը, որի անդամներն $a^l_j$ ակտիվացիաներն են։
    </p>

    <p>
      Այսպիսով,
      <span id="margin_735534017585_reveal" class="equation_link">(23)</span><span id="margin_735534017585" class="marginequation" style="display: none;"><a href="chap2.html#eqtn23" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_735534017585_reveal').click(function() {
          $('#margin_735534017585').toggle('slow', function() {});
        });
      </script>
      հավասարումն արտագրենք մատրիցային տեսքով, որի հիմքում ընկած է արդեն քննարկված ֆունկցիաների վեկտորացման գաղափարը։ Մեր նպատակն է, որպեսի $\sigma$-ն կարողանանք կիրառել $v$ վեկտորի բոլոր էլեմենտների վրա։ Ֆունկցիայի էլեմենտ-առ-էլեմենտ կիրառումը նշանակենք $\sigma(v)$
      արտահայտությամբ։ $\sigma(v)$-ի տարրերը պարզապես $\sigma(v)_j = \sigma(v_j)$ էլեմենտներն են։ Որպես օրինակ, դիտարկենք այն դեպքը, երբ ունենք $f(x) = x^2$ ֆունկցիան, ապա ֆունկցիայի վեկտորացված տեսքը կլինի՝
      <a class="displaced_anchor" name="eqtn24"></a>\begin{eqnarray} f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right) = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right] = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right], \tag{24}\end{eqnarray}
      այսինքն վեկտորացված $f$-ն ուղղակի վեկտորի յուաքանչյուր անդամը քառակուսի է բարձրացնում։
    </p>

    <p>
      Հաշվի առնելով այս բոլոր նշանակումները,
      <span id="margin_114817738681_reveal" class="equation_link">(23)</span><span id="margin_114817738681" class="marginequation" style="display: none;"><a href="chap2.html#eqtn23" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_114817738681_reveal').click(function() {
          $('#margin_114817738681').toggle('slow', function() {});
        });
      </script>
      հավասարումը կարելի է արտագրել այսպիսի գեղեցիկ և կոմպակտ վեկտորացված տեսքով.

      <a class="displaced_anchor" name="eqtn25"></a>\begin{eqnarray} a^{l} = \sigma(w^l a^{l-1}+b^l). \tag{25}\end{eqnarray} Այս արտահայտությունը ընդհանուր առմամբ հնարավորություն է տալիս դիտարկել նախորդ շերտի ակտիվացիաների ազդեցությունը հաջորդ շերտի ակտիվացիաների
      վրա. ուղղակի կիրառում ենք կշիռների մատրիցը ակտիվացիաների վրա, այնուհետև ավելացնում ենք շեղման վեկտորը և վերջապես կիրառում $\sigma$ ֆունկցիան
      <a id="quirk"></a>*<span class="marginnote">
  *Ի դեպ, այս արտահայտությունն է, որ մոտիվացնում է $w^l_{jk}$
  նշանակման ինդեքսների «տարօրինակ» հաջորդականությունը։ Եթե մենք
  օգտագործեինք $j$ ինդեքսը մուտքային նեյրոնի համար և $k$-ն ելքային
  նեյրոնի համար, ապա ստիպված կլինեինք
  <span id="margin_984031576396_reveal" class="equation_link">(25)</span><span id="margin_984031576396" class="marginequation" style="display: none;"><a href="chap2.html#eqtn25" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  a^{l} = \sigma(w^l a^{l-1}+b^l) \nonumber\end{eqnarray}</a>

  </span>
      <script>
        $('#margin_984031576396_reveal').click(function() {
          $('#margin_984031576396').toggle('slow', function() {});
        });
      </script>
      հավասարման մեջ կշիռների մատրիցը փոխարինել տրանսպոնացված մատրիցով։ Դա փոքր փոփոխություն է, բայց դժվարություններ առաջացնող և մենք կկորցնեինք այն պարզ մտավոր մոդելը, որը թույլ է տալիս օգտագործել «կշիռների մատրիցը կիրառենք ակտիվացիաների վրա» արտահայտությունը։</span>։
      Այսպիսի գլոբալ տեսքը հաճախ ավելի հեշտ է ընկալել և ավելի հակիրճ է (և պարունակում է ավելի քիչ ինդեքսներ) քան նեյրոն-առ-նեյրոն տեսքը, որը մենք դիտարկում էինք մինչ այժմ։ Դա ինդեքսային դժողքից խուսափելու մեխանիզմ է, որը միաժամանակ հստակություն է ապահովում
      այն առումով, թե առհասարակ ինչի մասին է գնում խոսքը։ Արտահայտությունն օգտակար է նաև պրակտիկ տեսանկյունից, քանի որ մատրիցային գրադարանների մեծ մասն ապահովում են մատրիցային բազմապատկման, վեկտորների գումարման և վեկտորացման արագագործ իրականացումներ։
      Նախորդ գլխում զետեղված
      <a href="chap1.html#implementing_our_network_to_classify_digits">կոդը</a> անուղղակիորեն օգտագործեց այս արատահայտությունը, որպեսզի հաշվարկի ցանցի վարքագիծը։
    </p>

    <p>
      Երբ
      <span id="margin_709882465331_reveal" class="equation_link">(25)</span><span id="margin_709882465331" class="marginequation" style="display: none;"><a href="chap2.html#eqtn25" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  a^{l} = \sigma(w^l a^{l-1}+b^l) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_709882465331_reveal').click(function() {
          $('#margin_709882465331').toggle('slow', function() {});
        });
      </script>
      հավասարումն օգտագործում ենք $a^l$-ը հաշվելու նպատակով, ապա հաշվում ենք նաև միջանկյալ $z^l \equiv w^l a^{l-1}+b^l$ արժեքը։ Այդ մեծությունը նշանակենք որպես $z^l$ ՝ $l$-րդ շերտի նեյրոնների <em>կշռված մուտքեր (weighted input)</em>։ Հետագայում այս գլխում
      նաև կտեսնեք $z^l$ կշռված մուտքերի կիրառությունը։
      <span id="margin_383003775830_reveal" class="equation_link">(25)</span><span id="margin_383003775830" class="marginequation" style="display: none;"><a href="chap2.html#eqtn25" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  a^{l} = \sigma(w^l a^{l-1}+b^l) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_383003775830_reveal').click(function() {
          $('#margin_383003775830').toggle('slow', function() {});
        });
      </script>
      հավասարումը երբեմն արտահայտվում է կշռված մուտքերի միջոցով՝ $a^l = \sigma(z^l)$։ Հարկ է նշել նաև, որ $z^l$-ի տարրերը $z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$ կամ պարզապես $z^l_j$-ը $l$-րդ շերտի $j$-րդ նեյրոնի ակտիվացիայի ֆունկցիայի կշռված մուտքն է։
    </p>

    <p>
      <h3><a name="the_two_assumptions_we_need_about_the_cost_function"></a>
    <a href="#the_two_assumptions_we_need_about_the_cost_function">Երկու ենթադրություններ գնային ֆունկցիայի վերաբերյալ</a>
  </h3>
    </p>

    <p>
      Հետադարձ տարածման նպատակն է հաշվել $C$ գնային ֆունկցիայի $\partial C / \partial w$ և $\partial C / \partial b$ մասնական ածանցյալները $w$ կշիռների և $b$ շեղումների նկատմամբ։ Կատարենք երկու ենթադրություններ հետադարձ տարածման համար։ Սակայն, մինչ այդ դիտարկենք
      քառակուսային գնային ֆունկցիան՝ սահմանված նախորդ գլխում (տես
      <span id="margin_59913254684_reveal" class="equation_link">(6)</span><span id="margin_59913254684" class="marginequation" style="display: none;"><a href="chap1.html#eqtn6" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2 \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_59913254684_reveal').click(function() {
          $('#margin_59913254684').toggle('slow', function() {});
        });
      </script>
      հավասարումը)։ Նախորդ գլխից հայտնի է, որ քառակուսային գնային ֆունկցիան ունի հետևյալ տեսքը՝
      <a class="displaced_anchor" name="eqtn26"></a>\begin{eqnarray} C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2, \tag{26}\end{eqnarray} որտեղ $n$-ը մարզման օրինակների քանակն է, իսկ գումարն ըստ անհատական $x$ մարզման օրինակների է, $y = y(x)$ համապատասխան
      ցանկալի ելքային արժեքն է և $a^L = a^L(x)$ ցանցի ելքային վեկտորն է $x$ մուտքային վեկտորի դեպքում։
    </p>

    <p>
      Այպիսով, ի՞նչ ենթադրություններ կարող ենք կատարել $C$ գնային ֆունկցիայի վերաբերյալ, որպեսզի հետադարձ տարածումը հնարավոր լինի կիրառել։ Առաջին ենտադրությունն այն է, որ գնային ֆունկցիան կարելի է արտահայտել $C = \frac{1}{n} \sum_x C_x$ հավասարմամբ որպես $x$
      մարզման օրինակներից կախված առանձին $C_x$ գնային ֆունկցիաների հանրահաշվական միջին։ Այս պնդումը ճիշտ է քառակուսային գնային ֆունկցիայի դեպքում, որտեղ $C_x = \frac{1}{2} \|y-a^L \|^2$ գնային ֆունկցիան է՝ կախված մեկ մարզման օրինակից։ Այս ենթադրությունը
      ճիշտ կլինի նաև մնացած այլ գնային ֆունկցիաների դեպքում, որ կհանդիպենք այս գրքում։
    </p>

    <p>
      Մենք այս ենթադրության կարիքն ունենք, քանի որ հետադարձ տարածումն, ըստ էության, մեզ հնարավորություն է տալիս հաշվել $\partial C_x / \partial w$ և $\partial C_x / \partial b$ մասկանակն ածանցյալները մեկ մարզման օրինակի համար։ Այնուհետև $\partial C / \partial
      w$ և $\partial C / \partial b$ արժեքները կվերականգնենք՝ հաշվելով միջինը բոլոր մարզման օրինակների երկայնքով։ Այսպիսով, հաշվի առնելով վերևում կատարված պնդումը, կարող ենք $C_x$ գնային ֆունկցիայի $x$ ինդեքսը դեն նետել և գնային ֆունկցյաին հղվել որպես
      $C$, քանի որ մասնական ածանցյալի դիտարկումը բերվեց ֆիքսված մուտքային վեկտորի դեպքում մասնական ածանցյալի հաշվմանը։ Վերջ ի վերջո $x$ ինդեքսը հետ կբերենք, սակայն նշանակումների պարզության համար այն առայժմ բաց կթողնենք։
    </p>

    <p>
      Գնային ֆունկցիայի մասին երկրորդ ենթադրությունը այն է, որ այն կարելի է արտահայտել որպես ֆունկցիա կախված նեյրոնային ցանցերի ելքային արժեքներից.
      <center>
        <img src="images/tikz18.png" />
      </center>
      Օրինակ, քառակուսային գնային ֆունկցիան բավարարում է այս պնդմանը, քանի որ քառակուսային գնային ֆունկցիան տրված $x$ մուտքային վեկտորի համար կարելի է արտահայտել որպես
      <a class="displaced_anchor" name="eqtn27"></a>\begin{eqnarray} C = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2, \tag{27}\end{eqnarray} որը ֆունկցիա է՝ կախված ելքային ակտիվացիաներից։ Իհարկե, գնային ֆունկցիան կախված է նաև $y$ ցանկալի
      ելքային արժեքից և հարց է առաջանում, թե ինչու՞ չենք դիտարկում գնային ֆունկցիան որպես $y$-ից կախված ֆունկցիա։ Նկատենք, որ մուտքային մարզման օրինակը ֆիքսված է, հետևաբար ֆիքսված է նաև $y$ պարամետրը։ Այդ արժեքն, ըստ էության կախված չէ կշիռներից և շեղումներից,
      այսինքն կշիռների և շեղումների փոփոխության դեպքում այն չի փոխվի, հետևաբար դա այն չէ, ինչ նեյրոնային ցանցը սովորում է։ Այսպիսով, $C$-ն կարող ենք դիտարկել որպես $a^L$ ելքային վեկտորներից կախված ֆունկցիա, որտեղ $y$ պարզապես պարամետր է, որը մասնակցում
      է ֆունկցիայի սահմանմանը։
    </p>

    <p></p>
    <p></p>
    <p></p>

    <p>
      <h3>
    <a name="the_hadamard_product_$s_\odot_t$"></a>
    <a href="#the_hadamard_product_$s_\odot_t$">Հադամարի արտադրյալը՝ $s \odot t$</a>
  </h3>
    </p>

    <p>
      Հետադարձ տարածման ալգորիթմը հիմնված է որոշ գծային հանրահաշվի գործողությունների վրա՝ վեկտորների գումարում, վեկտորի բազմապատկում մատրիցով և այլն։ Սակայն գործողություններից մեկն ավելի քիչ հաճախ է օգտագործվում քան մնացածները։ Ենթադրենք, որ $s$ և $t$ միևնույն
      չափողականությամբ վեկտորներ են։ Վեկտորների <em>էլեմենտ առ էլեմենտ</em> արտադրյալը նշանակենք $s \odot t$ արտահայտությամբ։ Հետևաբար $s \odot t$ արտադրյալի տարրերը կլինեն $(s \odot t)_j = s_j t_j$ արժեքները։ Դիտարկենք հետևյալ օրինակը.

      <a class="displaced_anchor" name="eqtn28"></a>\begin{eqnarray} \left[\begin{array}{c} 1 \\ 2 \end{array}\right] \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right] = \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right] = \left[ \begin{array}{c}
      3 \\ 8 \end{array} \right]. \tag{28}\end{eqnarray} Այս էլեմենտ առ էլեմենտ արտադրյալը այլ կերպ անվանում են <em>Հադամարի արտադրյալ</em> կամ <em>Շուրի արտադրյալ</em>։ Մենք կօգտագործենք Հադամարի արտադրյալ տերմինը։ Լավ մատրիցային գրադարանները սովորաբար
      տրամադրում են արագագործ Հադամարի արտադրյալի իրականացում, որը բավականին օգտակար է հետադարձ տարածումն իրականացնելիս։
    </p>

    <p>
      <h3>
    <a name="the_four_fundamental_equations_behind_backpropagation"></a>
    <a href="#the_four_fundamental_equations_behind_backpropagation">Հետադարձ տարածման հիմքում ընկած չորս հիմնական հավասարումները</a>
  </h3>
    </p>

    <p>
      Հետադարձ տարածումն այն մասին է, թե ինչպես է ցանցի գնային ֆունկցիան փոփոխվում՝ կախված կշիռների և շեղումների փոփոխություններից։ Սա իր հերթին նշանակում է, որ պետք է հաշվել $\partial C / \partial w^l_{jk}$ և $\partial C / \partial b^l_j$ մասնակի ածանցյալները։
      Նախ ներմուծենք $\delta^l_j$ միջանկյալ մեծությունը, որը կկոչենք $l^{\rm րդ}$ շերտի $j^{\rm րդ}$ նեյրոնի <em>սխալանք (error)</em>։ Հետադարձ տարածումը ցույց կտա որոշակի պրոցեդուրա $\delta^l_j$ սխալանքը հաշվելու համար, այնուհետև $\delta^l_j$-ն
      կկապենք $\partial C / \partial w^l_{jk}$ և $\partial C / \partial b^l_j$ մասնակի ածանցյալների հետ։
    </p>

    <p>
      Որպեսզի հասկանանք, թե ինչպես է սխալանքը սահմանումը, պատկերացնենք, որ մեր նեյրոնային ցանցում սատանա է հայտնվել։
      <center>
        <img src="images/tikz19.png" />
      </center>
      Այն տեղակայված է $l$-րդ շերտի $j^{\rm րդ}$ նեյրոնում։ Սատանան յուրաքանչյուր մուտքային արժեքի համար խառնաշփոթ է ստեղծում այդ նեյրոնի աշխատանքում։ Այն նեյրոնի կշռված մուտքերին (weighted input) ավելացնում է փոքրիկ $\Delta z^l_j$ փոփոխություն այնպես, որ ելքում
      $\sigma(z^l_j)$ արժեքը ստանալու փոխարեն ստացվում է $\sigma(z^l_j+\Delta z^l_j)$։ Այս փոփոխությունը, իհարկե, տարածվում է ցանցի հետագա շերտերում՝ հանգեցնելով գնային ֆունկցիայի արժեքի $\frac{\partial C}{\partial z^l_j} \Delta z^l_j$ փոփոխության։
    </p>

    <p>
      Հարկ է նշել, որ այդ սատանան ունի բարի նպատակներ և փորձում է օգնել ձեզ գինը բարելավել, օրինակ, գտնել այնպիսի $\Delta z^l_j$, որի արդյունքում գինը կնվազի։ Ենթադրենք $\frac{\partial C}{\partial z^l_j}$ մասնակի ածանցյալն ունի բավականին մեծ արժեք (դրական կամ
      բացասական)։ Սատանան կարող է բավականին իջեցնել գինը, ընտրելով այնպիսի $\Delta z^l_j$, որը հակառակ նշանի է $\frac{\partial C}{\partial z^l_j}$ մեծության նկատմամբ։ Ընդ որում, եթե $\frac{\partial C}{\partial z^l_j}$ մոտ է զրոյին, ապա սատանան էապես չի
      կարող ազդել գնի բարելավման հարցում՝ «խառնաշփոթ» առաջացնելով $z^l_j$ կշռված մուտքին։ Եվ այդ դեպքում սատանան կհանգի այն եզրակացության, որ նեյրոնն արդեն մոտ է օպտիմալ լինելուն*
      <span class="marginnote">
    * Սա ճիշտ է միայն փոքր $\Delta z^l_j$ փոփոխությունների դեպքում, իհարկե։
    Մենք կենթադրենք, որ սատանան սահմանափակված է այդպիսի փոքր փոփոխություններ
    անելուն։
  </span>։ Այսպիսով, ինտուիտիվ կարելի է հասկանալ, որ $\frac{\partial C}{\partial z^l_j}$ մասնակի ածանցյալը նեյրոնում առկա սխալանքի համար որոշակի չափ է հանդիսանում։
    </p>

    <p>
      Այս պատմությունը ոգեշնչում է, որպեսզի $l$-րդ շերտի $j$-րդ նեյրոնի $\delta^l_j$ սխալանքը սահմանենք որպես

      <a class="displaced_anchor" name="eqtn29"></a>\begin{eqnarray} \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}. \tag{29}\end{eqnarray} Ինչպես մնացած դեպքերում, $\delta^l$-ով կնշանակենք $l$-րդ շերտի սխալանքների վեկտորը։ Հետադարձ տարածումը մեզ
      հնարավորություն կտա, որպեսզի բոլոր շերտերի համար հաշվենք $\delta^l$ մեծությունը, այնուհետև այդ սխալանքները կապենք այն արժեքների հետ, որոնք իրապես մեր հետաքրքրությունների շրջանակներիում են՝ $\partial C / \partial w^l_{jk}$ և $\partial C / \partial
      b^l_j$։
    </p>

    <p>
      Հարց է առաջանում, թե ինչու է սատանան փոփոխում $z^l_j$ կշռված մուտքի արժեքը։ Բնական կլիներ, որ սատանան փոփոխեր $a^l_j$ ակտիվացիայի արժեքը, ինչի արդյունքում $\frac{\partial C}{\partial a^l_j}$ մեծությունը կօգտագործեինք սխալանքը չափելու նպատակով։ Ըստ էության,
      եթե այդ ուղղությամբ գնանք, ապա կստանանք նպանատիպ արդյունք, որը նկարագրված է ներքևում։ Սակայն տարբերությունն այն է, որ այս մոտեցման դեպքում հետադարձ տարածման մաթեմատիկական ներկայացումն ավելի բարդ տեսք ունի։ Հետևաբար կաշխատենք $\delta^l_j = \frac{\partial
      C}{\partial z^l_j}$ մեծության հետ որպես սխալանքի չափման գործիք*
      <span class="marginnote">
    * Այնպիսի դասակարգման խնդիրներում, ինչպիսին է MNIST-ը, «սխալանք»
    տերմինը սովորաբար օգտագործվում է դասակարգման ձախողման գործակցի
    իմաստով։ Օրինակ, երբ նեյրոնային ցանցը թվանշանների 96.0 տոկոսը ճիշտ
    է դասակարգում, ապա ձախողման գործակիցը 4.0 տոկոս է։ Իհարկե, $\delta$
    վեկտորների իմաստը փոքր-ինչ այլ է, սակայն պրակտիկորեն դժվար չի լինի
    հասկանալ իմաստների տարբերությունը կախված կոնտեքստից։
  </span>։
    </p>

    <p>
      <strong>Հարձակման պլանը։</strong> Հետադարձ տարածումը հիմնված է 4 ֆունդամենտալ հավասարումների վրա։ Այդ հավասարումները միասին մեզ հնարավորություն են տալիս հաշվել $\delta^l$ սխալանքն ու գնային ֆունկցիայի գրադիենտը։ 4 հավասարումները կնկարագրենք ավելի
      ուշ, սակայն պետք չէ սպասել, որ անմիջապես հեշտությամբ կընկալեք, հակառակ դեպքում կարող է հիասթափություն ապրեք։ Ըստ էության, հետադարձ տարածման հավասարումներն այնքան հարուստ են, որ իրենց լավ հասկանալու համար կպահանջվի բավականին ժամանակ և համբերություն՝
      աստիճանաբար ավելի խորանալու համար։ Լավ նորությունն այն է, որ այդպիսի համբերատարությունը բազմակատիկ վարձատրվում է։ Եվ այսպիսով, քննարկումներն այս բաժնում միայն սկիզբն են, որպեսզի օգնեն խորությամբ հասկանալ և ճանապարհը հարթեն։
    </p>

    <p>
      Ահա նախադիտում (preview) այն մասին, թե ինչպես այս գլխում կխորանանք հավասարումների մեջ։ Ես
      <a href="chap2.html#proof_of_the_four_fundamental_equations_(optional)">
    կտամ հավասարումներին կարճ ապացույցներ
  </a>, որ կօգնի բացատրել, թե ինչու իրենք տեղի ունեն։ Այնուհետև մենք
      <a href="chap2.html#the_backpropagation_algorithm">
    կվերաձևակերպենք հավասարումները
  </a> ալգորիթմական տեսքով որպես պսեվդոկոդ և

      <a href="chap2.html#the_code_for_backpropagation">կտեսնենք, թե ինչպես</a> կարելի է պսեվդոկոդն իրականացնել որպես աշխատող Python կոդ։

      <a href="chap2.html#backpropagation_the_big_picture">
    Գլխի վերջնական բաժնում
  </a> կկառուցենք հետադարձ տարածման հավասարումների ինտուիտիվ պատկերացում և կխոսենք այն մասին, թե ինչպես կարելի էր զրոյից հայտնագործել իրենց։ Ամբողջ ընթացքում շարունակաբար կվերադառնանք այդ չորս հիմնական հավասարումներին և ավելի խորը հասկանալուն զուգընթաց
      հավասարումներն ավելի հարմարավետ կդառնան, և, հնարավոր է, որ նույնիսկ գեղեցիկ և բնական։
    </p>

    <p>
      <strong>Ելքային շերտի սխալանքի $\delta^L$ հավասարումը։</strong> $\delta^L$ հավասարման էլեմենտներն ունեն հետևյալ տեսքը՝

      <a class="displaced_anchor" name="eqtnBP1"></a>\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j). \tag{BP1}\end{eqnarray} Դիտարկենք այդ արտահայտությունը։ Հավասարման աջ կողմի առաջին հատվածը՝ $\partial C / \partial a^L_j$-ը
      պարզապես ցույց է տալիս, թե ինչ արագությամբ է գնային ֆունկցիան փոփոխվում՝ կախված $j^{\rm րդ}$ ելքային ակտիվացիայից։ Օրինակ, եթե $C$-ն էապես կախված չէ տրված $j$ ելքային նեյրոնից, ապա $\delta^L_j$ կլինի բավականին փոքր՝ այն ինչ մենք սպասում էինք։ Աջակողմյան
      երկրորդ արտահայտությունը ցույց է տալիս $\sigma$ ակտիվացիայի ֆունկցիայի փոփոխման արագությունը՝ կախված $z^L_j$-ից։
    </p>

    <p>
      Նկատենք, որ

      <span id="margin_919611304299_reveal" class="equation_link">(BP1)</span>
      <span id="margin_919611304299" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_919611304299_reveal').click(function() {
          $('#margin_919611304299').toggle('slow', function() {});
        });
      </script>

      արտահայտության բոլոր անդամները հեշտությամբ կարելի է հաշվել։ Հատկապես $z^L_j$-ը հաշվարկվում է ցանցի վարքագիծը հաշվելու ընթացքում և $\sigma'(z^L_j)$ հաշվարկումը պարզապես լրացուցիչ ոչ մեծ «գլխացավանք» է։ $\partial C / \partial a^L_j$ մասնական ածանցյալի տեսքն
      իհարկե կախված է գնային ֆունկցիայի տեսքից։ Սակայն, հաշվի առնելով, որ գնային ֆունկցիան հայտնի է, ապա $\partial C / \partial a^L_j$ հաշվարկումը նույնպես իրենից մեծ խնդիր չի ներկայացնում։ Օրինակ, եթե օգտագործում ենք քառակուսային գնային ֆունկցիան, ապա
      $C = \frac{1}{2} \sum_j (y_j-a^L_j)^2$, հետևաբար $\partial C / \partial a^L_j = (a_j^L-y_j)$ ինչը հեշտությամբ հաշվարկելի է։
    </p>

    <p>
      <span id="margin_919611304299_reveal" class="equation_link">(BP1)</span>
      <span id="margin_919611304299" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_919611304299_reveal').click(function() {
          $('#margin_919611304299').toggle('slow', function() {});
        });
      </script>

      հավասարումը սահմանում է $\delta^L$ վեկտորի անդամները։ Հավասարումը չունի վեկտորական տեսք, ինչն անհրաժեշտ է հետադարձ տարածման համար, այնուամենայնիվ այն ճշգրիտ նկարագրում է ելքային շերտի սխալանքը։ Այսպիսով, $\delta^L$ արտահայտությունը կարելի է արտագրել մատրիցային
      տեսքով հետևյալ կերպ՝

      <a class="displaced_anchor" name="eqtnBP1a"></a>\begin{eqnarray} \delta^L = \nabla_a C \odot \sigma'(z^L). \tag{BP1a}\end{eqnarray} Որտեղ $\nabla_a C$ սահմանվում է որպես վեկտոր, որի անդամներն են $\partial C / \partial a^L_j$ մասնական ածանցյալները։
      $\nabla_a C$-ն կարելի է ընկալել որպես $C$ գնային ֆունկիայի փոփոխման գործակիցը՝ կախված ելքային ակտիվացիաներից։ Հեշտ է նկատել, որ

      <span id="margin_570480201049_reveal" class="equation_link">(BP1a)</span>
      <span id="margin_570480201049" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1a" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \delta^L = \nabla_a C \odot \sigma'(z^L) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_570480201049_reveal').click(function() {
          $('#margin_570480201049').toggle('slow', function() {});
        });
      </script>

      և

      <span id="margin_983733590680_reveal" class="equation_link">(BP1)</span>
      <span id="margin_983733590680" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_983733590680_reveal').click(function() {
          $('#margin_983733590680').toggle('slow', function() {});
        });
      </script>

      հավասարումները համարժեք են։ Այդ պատճառով հաճախակի կօգտագործենք

      <span id="margin_506574502658_reveal" class="equation_link">(BP1)</span>
      <span id="margin_506574502658" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_506574502658_reveal').click(function() {
          $('#margin_506574502658').toggle('slow', function() {});
        });
      </script>

      հավասարումը։ Օրինակ, քառակուսային գնի ֆունկցիայի դեպքում $\nabla_a C = (a^L-y)$, հետևաբար

      <span id="margin_365533411234_reveal" class="equation_link">(BP1)</span>
      <span id="margin_365533411234" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_365533411234_reveal').click(function() {
          $('#margin_365533411234').toggle('slow', function() {});
        });
      </script>

      հավասարման մատրիցային տեսքը կլինի

      <a class="displaced_anchor" name="eqtn30"></a>\begin{eqnarray} \delta^L = (a^L-y) \odot \sigma'(z^L). \tag{30}\end{eqnarray} Ինչպես տեսնում եք, այս հավասարումն ունի վեկտորական տեսք և այն կարելի է հեշտությամբ հաշվել՝ օգտագործելով Numpy գրադարանը։
    </p>

    <p>
      <strong>
    $\delta^l$ սխալանաքի արտահայտումն ըստ հաջորդ շերտի $\delta^{l+1}$
    սխալանքի։
  </strong> Դիտարկենք հետևյալ հավասարումը.

      <a class="displaced_anchor" name="eqtnBP2"></a>\begin{eqnarray} \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l), \tag{BP2}\end{eqnarray} որտեղ $(w^{l+1})^T$ կշիռների $w^{l+1}$ տրանսպոնացված մատրիցն է՝ $(l+1)^{\rm րդ}$ շերտի համար։ Այս հավասարումը
      կարող է բարդացված թվալ, սակայն յուրաքանչյուր անդամ ունի գեղեցիկ մեկնաբանություն։ Ենթադրենք, որ $(l+1)^{\rm րդ}$ շերտի $\delta^{l+1}$ սխալանքը հայտնի է։ $(w^{l+1})^T$ տրանսպոնացված կշիռների մատրիցի կիրառումը ինտուիտիվ կարելի է ընկալել որպես սխալանքը
      ցանցում <em>հետադարձ</em> շարժելու գործողություն, որը տալիս է $l^{\rm րդ}$ շերտի ելքային արժեքի սխալանքի որոշակի չափողականություն։ Այնուհետև կիրառում ենք $\odot \sigma'(z^l)$ Հադամարի արտադրյալը։ Այս գործողությամբ սխալանքը հետադարձ տեղափոխվում է
      $l$ շերտի ակտիվացիայի ֆունկցիային, որի արդյունքում ստանում ենք $\delta^l$ սխալանքը $l$ շերտի կշռված մուտքերի նկատմամբ։
    </p>

    <p>
      Միավորելով

      <span id="margin_520088358076_reveal" class="equation_link">(BP2)</span>
      <span id="margin_520088358076" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}
    </a>
  </span>
      <script>
        $('#margin_520088358076_reveal').click(function() {
          $('#margin_520088358076').toggle('slow', function() {});
        });
      </script>

      և

      <span id="margin_853208206649_reveal" class="equation_link">(BP1)</span>
      <span id="margin_853208206649" class="marginequation" style="display: none;"><a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_853208206649_reveal').click(function() {
          $('#margin_853208206649').toggle('slow', function() {});
        });
      </script>

      հավասարումները, կարող ենք $\delta^l$ սխալանքը հաշվել ցանցի կամայական շերտում։ Կսկսենք $\delta^L$-ի հաշվումից՝ օգտագործելով

      <span id="margin_569252953000_reveal" class="equation_link">(BP1)</span>
      <span id="margin_569252953000" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a>
  </span>
      <script>
        $('#margin_569252953000_reveal').click(function() {
          $('#margin_569252953000').toggle('slow', function() {});
        });
      </script>

      հավասարումը, այնուհետև կօգտագործենք

      <span id="margin_646471794768_reveal" class="equation_link">(BP2)</span>
      <span id="margin_646471794768" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_646471794768_reveal').click(function() {
          $('#margin_646471794768').toggle('slow', function() {});
        });
      </script>

      հավասարումը, որպեսզի հաշվենք $\delta^{L-1}$ սխալանքը, այնուհետև կկրկնենք

      <span id="margin_58635688739_reveal" class="equation_link">(BP2)</span>
      <span id="margin_58635688739" class="marginequation" style="display: none;"><a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_58635688739_reveal').click(function() {
          $('#margin_58635688739').toggle('slow', function() {});
        });
      </script>

      հավասարման կիրառումը, որպեսզի հաշվենք $\delta^{L-2}$ և ատդպես շարունակ մինչև ցանցի «սկիզբը»
    </p>

    <p>
      <strong>
    Գնային ֆունկցիայի փոփոխման գործակցի հավասարումը՝ կախված ցանցի շեղումներից։
  </strong> Դիտարկենք հետևյալ հավասարումը.

      <a class="displaced_anchor" name="eqtnBP3"></a>\begin{eqnarray} \frac{\partial C}{\partial b^l_j} = \delta^l_j. \tag{BP3}\end{eqnarray} Այն է, $\delta^l_j$ սխալանքը <em>ճիշտ նույնն</em> է, ինչ $\partial C / \partial b^l_j$ փոփոխման գործակիցը։ Սա
      հրաշալի նորություն է, քանի որ

      <span id="margin_8280399999_reveal" class="equation_link">(BP1)</span>
      <span id="margin_8280399999" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_8280399999_reveal').click(function() {
          $('#margin_8280399999').toggle('slow', function() {});
        });
      </script>

      և

      <span id="margin_557075118630_reveal" class="equation_link">(BP2)</span>
      <span id="margin_557075118630" class="marginequation" style="display: none;"><a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_557075118630_reveal').click(function() {
          $('#margin_557075118630').toggle('slow', function() {});
        });
      </script>

      արդեն ցույց են տվել, թե ինչպես կարելի է հաշվել $\delta^l_j$ սխալանքը։ Կարող ենք

      <span id="margin_205000449729_reveal" class="equation_link">(BP3)</span>
      <span id="margin_205000449729" class="marginequation" style="display: none;"><a href="chap2.html#eqtnBP3" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =
  \delta^l_j \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_205000449729_reveal').click(function() {
          $('#margin_205000449729').toggle('slow', function() {});
        });
      </script>

      հավասրումն արտագրել կարճ որպես

      <a class="displaced_anchor" name="eqtn31"></a>\begin{eqnarray} \frac{\partial C}{\partial b} = \delta, \tag{31}\end{eqnarray} որտեք $\delta$ հաշվարկում ենք նույն նեյրոնի համար, ինչի համար դիտարկում էինք $b$ շեղումը։
    </p>

    <p>
      <strong>
    Գնային ֆունկցիայի փոփոխման գործակցի հավասարումը՝
    կախված ցանցի կշիռներից։
  </strong> Դիտարկենք հետևյալ հավասարումը՝

      <a class="displaced_anchor" name="eqtnBP4"></a>\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j. \tag{BP4}\end{eqnarray} Այն ցույց է տալիս, թե ինչպես կարելի է հաշվել $\partial C / \partial w^l_{jk}$ մասնական ածանցյալներն
      ըստ $\delta^l$ և $a^{l-1}$ մեծությունների, ինչը մենք արդեն գիտենք հաշվել։ Հավասարումը կարելի է արտահայտել ավելի կարճ տեսքով հետևյալ կերպ.

      <a class="displaced_anchor" name="eqtn32"></a>\begin{eqnarray} \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out}, \tag{32}\end{eqnarray} որտեղ $a_{\rm in}$ մեծությունը $w$ կշիռների հետ միասին մուտք հանդիսացող նեյրոնի ակտիվացիան է և $\delta_{\rm
      out}$-ն նեյրոնի արդյունքի ելքային սխալանքն է $w$ կշիռների դեպքում։ Մոտիկից դիտելով $w$ կշռին և այն երկու նեյրոններին, որոնք կապակցված են այդ կշռով, կտեսնենք հետևյալ պատկերը
      <center>
        <img src="images/tikz20.png" />
      </center>

      <span id="margin_988156473660_reveal" class="equation_link">(32)</span>
      <span id="margin_988156473660" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn32" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial
    C}{\partial w} = a_{\rm in} \delta_{\rm out} \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_988156473660_reveal').click(function() {
          $('#margin_988156473660').toggle('slow', function() {});
        });
      </script>

      հավասարման գեղեցիկ հետևանք է այն, որ երբ $a_{\rm in}$ փոքր է, $a_{\rm in} \approx 0$, ապա $\partial C / \partial w$ գրադիենտի արժէքը նույնպես կձգտի փոքր արժեքների։ Այդ դեպքում կասենք, որ կշիռը
      <em>դանդաղ է սովորում</em>, ինչը նշանակում է, որ այն շատ չի փոփոխվում գրադիենտային վայրէջքի ժամանակ։ Այլ կերպ ասած

      <span id="margin_30338286679_reveal" class="equation_link">(BP4)</span>
      <span id="margin_30338286679" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_30338286679_reveal').click(function() {
          $('#margin_30338286679').toggle('slow', function() {});
        });
      </script>

      հավասարման հետևանքներից մեկն այն է, որ թույլ ակտիվացիայով (low-activation) նեյրոններից դուրս եկող կշիռները դանդաղ են սովորում։
    </p>

    <p><a name="saturation"></a></p>

    <p>
      <span id="margin_250484114753_reveal" class="equation_link">(BP1)</span>
      <span id="margin_250484114753" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_250484114753_reveal').click(function() {
          $('#margin_250484114753').toggle('slow', function() {});
        });
      </script>
      -
      <span id="margin_282300343827_reveal" class="equation_link">(BP4)</span>
      <span id="margin_282300343827" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_282300343827_reveal').click(function() {
          $('#margin_282300343827').toggle('slow', function() {});
        });
      </script>. հավասարումներից կարելի է անել նաև այլ հետևություններ։ Դիտարկենք $\sigma'(z^L_j)$ արտահայտությունը

      <span id="margin_16544494861_reveal" class="equation_link">(BP1)</span>
      <span id="margin_16544494861" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_16544494861_reveal').click(function() {
          $('#margin_16544494861').toggle('slow', function() {});
        });
      </script>. հավասարման մեջ։ Հիշելով

      <a href="chap1.html#sigmoid_graph">
    նախկին գլխում ներկայացված սիգմոիդ ֆունկցիայի գրաֆիկը
  </a> որտեղ $\sigma$ ֆունկցիայի աճը շատ փոքրանում է երբ $\sigma(z^L_j)$ մոտենում է $0$ կամ $1$ արժեքներին։ Եվ այսպիսով, հետևությունն այն է, որ վերջին շերտի կշիռները դանդաղ կսովորեն, եթե ելքային նեյրոնն ունի թույլ ակտիվացիա ($\approx 0$) կամ ուժեղ
      ակտիվացիա ($\approx 1$)։ Այս դեպքում ասում են, որ ելքային նեյրոնը <em>հագեցած (saturated)</em> է և արդյունքում կշիռը դադարել է սովորել (կամ սովորում է շատ դանդաղ)։ Նմանատիպ պնդումներ կարելի է անել նաև ելքային նեյրոնի շեղումների մասին։
    </p>

    <p>
      Նմանատիպ հետևություններ կարող ենք կատարել նաև միջանկյալ շերտերի դեպքում։ Դիտարկենք $\sigma'(z^l)$ արտահայտությունը
      <span id="margin_998669428161_reveal" class="equation_link">(BP2)</span>
      <span id="margin_998669428161" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_998669428161_reveal').click(function() {
          $('#margin_998669428161').toggle('slow', function() {});
        });
      </script>. հավասարման մեջ։ Պարզ է, որ $\delta^l_j$ ավելի հավանական է, որ փոքր լինի, եթե նեյրոնը մոտ է հագեցմանը։ Եվ սա, իր հերթին նշանակում է, որ յուրաքանչյուր կշիռների մուտք հագեցած նեյրոնին կսովորի դանդաղորեն*
      <span class="marginnote">
    *Այս տրամաբանությունը տեղի չունի, երբ ${w^{l+1}}^T \delta^{l+1}$
    արտահայտության արժեքն այնքան մեծ է, որը կոմպենսացնում է $\sigma'(z^l_j)$
    փոքր լինելը։ Այստեղ խոսքը գնում է ընդհանուր տենդենցների մասին։
  </span>.
    </p>

    <p>
      Այսպիսով, մենք սովորեցինք, որ կշիռը կսովորի դանդաղորեն, եթե մուտքային նեյրոնը թուլ ակտիվացիայով է կամ երբ ելքային նեյրոնը հագեցած է, այն է ունի ուժեղ կամ թույլ ակտիվացիա։
    </p>

    <p>
      Այս դիտարկումներից ոչ մեկը չապազանց զարմանալի չէ։ Այնուամենայնիվ, դրանք նպաստում են, որպեսզի բարելավենք մեր մտավոր մոդելը այն մասին, թե ինչ է տեղի ունենում, երբ նեյրոնային ցանցը սովորում է։ Ավելին, մենք կարող ենք այս դիտարկումներն ընդհանրացնել։ Պարզվում
      է, որ չորս ֆունդամենտալ հավասարումները տեղի ունեն ոչ միայն կամայական ակտիվացիայի դեպքում, այլ ոչ միայն սիգմոիդ ֆունկցիայի (քիչ ուշ ապացույցներում կտեսնենք, որ $\sigma$ ֆունկիայի ոչ մի հատկություն չի օգտագործվում)։ Եվ այդպիսով, կարող ենք այդ հավասարումներն
      օգտագործել, որպեսզի <em>նախագծենք</em> այնպիսի ակտիվացիայի ֆունկցիաներ, որոնք ունեն որոշակի ցանկալի «ուսուցման հատկություններ»։ Օրինակ, պատկերացնելու համար, ենթադրենք, որ պետք է ընտրենք այնպիսի (ոչ սիգմոիդ) ակտիվացիայի ֆունկցիա $\sigma$, որ
      $\sigma'$ ածանցյալը միշտ դրական է և երբեք զրոյին չի մոտենում։ Դա կարող է խոչընդոտել ուսուցման դանդաղեցմանը, որը տեղի է ունենում, երբ սիգմոիդ նեյրոնները հագենում են։ Ավելի ուշ գրքում կտեսնենք օրինակներ, երբ այդպիսի փոփոխություններ են արվում ակտիվացիայի
      ֆունկցիային։ Մտապահելով

      <span id="margin_678414648914_reveal" class="equation_link">(BP1)</span>
      <span id="margin_678414648914" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_678414648914_reveal').click(function() {
          $('#margin_678414648914').toggle('slow', function() {});
        });
      </script>

      -

      <span id="margin_936017178156_reveal" class="equation_link">(BP4)</span>
      <span id="margin_936017178156" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}</a></span>
      <script>
        $('#margin_936017178156_reveal').click(function() {
          $('#margin_936017178156').toggle('slow', function() {});
        });
      </script>

      հավասարումները, կօգնի բացատրել, թե ինչու են նմանատիպ փոփոխությունները փորձարկվում և ինչ ազդեցություն կարող են ունենալ։
      <p>
        <a name="backpropsummary"></a>
      </p>

      <p>
        <center>
          <img src="images/tikz21.png" />
        </center>
      </p>

      <p>
        <a id="alternative_backprop"></a>
      </p>

      <p>
        <h4>
    <a name="problem_567109"></a>
    <a href="#problem_567109">Խինդիր</a>
  </h4>
        <ul>
          <li>
            <strong>
        Հետադարձ տարածման հավասարումների այլընտրանքային ներկայացում.
      </strong> Հետադարձ տարածման հավասարումները (ավելի հստակ
            <span id="margin_857011554527_reveal" class="equation_link">(BP1)</span>
            <span id="margin_857011554527" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
          \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_857011554527_reveal').click(function() {
                $('#margin_857011554527').toggle('slow', function() {});
              });
            </script>

            և
            <span id="margin_446828221170_reveal" class="equation_link">(BP2)</span>
            <span id="margin_446828221170" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
          \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_446828221170_reveal').click(function() {
                $('#margin_446828221170').toggle('slow', function() {});
              });
            </script>

            հավասարումները) սկսեցինք օգտագործելով Հադամարի արտադրյալը։ Եթե ծանոթ չեք Հադամարի արտադրյալին, ապա այդ ներկայացումը կարող է շփոթեցնել։ Գոյություն ունի այլընտրանքային մոտեցում՝ հիմնված մատրիցների բազմապատկման վրա, որը որոշ ընթերցողներ կարող են բավականին
            ուսուցողական համարել։ (1) Ցույց տվեք, որ

            <span id="margin_273825325358_reveal" class="equation_link">(BP1)</span>
            <span id="margin_273825325358" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
          \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_273825325358_reveal').click(function() {
                $('#margin_273825325358').toggle('slow', function() {});
              });
            </script>

            հավասարումը կարող ենք արտահայտել հետևյալ կերպ՝
            <a class="displaced_anchor" name="eqtn33"></a>\begin{eqnarray} \delta^L = \Sigma'(z^L) \nabla_a C, \tag{33}\end{eqnarray} որտեղ $\Sigma'(z^L)$ քառակուսային մատրից է, որի անկյունագծային տարրերն ունեն $\sigma'(z^L_j)$ արժեքները և մնացած անդամներն
            ունեն զրոյական արժեք։ Նկատտենք, որ այդ մատրիցը կիրառվում է $\nabla_a C$-ի վրա որպես մատրիցային բազմապատկում։ (2) Ցույց տվեք, որ

            <span id="margin_592545063504_reveal" class="equation_link">(BP2)</span>
            <span id="margin_592545063504" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
      \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_592545063504_reveal').click(function() {
                $('#margin_592545063504').toggle('slow', function() {});
              });
            </script>

            կարելի է ներկայացնել որպես

            <a class="displaced_anchor" name="eqtn34"></a>\begin{eqnarray} \delta^l = \Sigma'(z^l) (w^{l+1})^T \delta^{l+1}. \tag{34}\end{eqnarray} (3) Միավորելով (1) և (2) խնդիրները, ցույց տվեք, որ
            <a class="displaced_anchor" name="eqtn35"></a>\begin{eqnarray} \delta^l = \Sigma'(z^l) (w^{l+1})^T \ldots \Sigma'(z^{L-1}) (w^L)^T \Sigma'(z^L) \nabla_a C \tag{35}\end{eqnarray} Այն ընթերցողները, որոնք հարմարավետ են իրենց զգում մատրիցային
            բազմապատկումների հետ, ապա այս հավասարումն ավելի հեշտ ընկալեն, քան

            <span id="margin_211286309191_reveal" class="equation_link">(BP1)</span>
            <span id="margin_211286309191" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
      \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_211286309191_reveal').click(function() {
                $('#margin_211286309191').toggle('slow', function() {});
              });
            </script>

            և

            <span id="margin_33317062381_reveal" class="equation_link">(BP2)</span>
            <span id="margin_33317062381" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
      \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_33317062381_reveal').click(function() {
                $('#margin_33317062381').toggle('slow', function() {});
              });
            </script>

            հավասարումները։

            <span id="margin_350024952201_reveal" class="equation_link">(BP1)</span>
            <span id="margin_350024952201" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
      \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_350024952201_reveal').click(function() {
                $('#margin_350024952201').toggle('slow', function() {});
              });
            </script>

            և

            <span id="margin_968208779254_reveal" class="equation_link">(BP2)</span>
            <span id="margin_968208779254" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
      \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_968208779254_reveal').click(function() {
                $('#margin_968208779254').toggle('slow', function() {});
              });
            </script>

            հավասարումների վրա կենտրոնանալու նպատակն այն է, որ այդ մոտեցման իրականացումն ավելի արագ է թվային առումով։
        </ul>
      </p>


      <p>
        <h3>
    <a name="proof_of_the_four_fundamental_equations_(optional)"></a>
    <a href="#proof_of_the_four_fundamental_equations_(optional)">Չորս հիմնական հավասարումների ապացույցները (ընտրովի)</a>
  </h3>
      </p>

      <p>
        Ապացուցենք չորս հիմնական հավասարումները՝

        <span id="margin_322535054922_reveal" class="equation_link">(BP1)</span>
        <span id="margin_322535054922" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_322535054922_reveal').click(function() {
            $('#margin_322535054922').toggle('slow', function() {});
          });
        </script>

        -

        <span id="margin_58831286370_reveal" class="equation_link">(BP4)</span>
        <span id="margin_58831286370" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_58831286370_reveal').click(function() {
            $('#margin_58831286370').toggle('slow', function() {});
          });
        </script>

        ։ Չորս հավասարումներն էլ հետևում են շատ փոփոխականի հաշվում բարդ ֆունկցիայի ածանցյալի կանոնից։ Եթե լավ եք տիրապետում բարդ ֆունկցիայի ածանցյալի կանոնին, ապա խորհուրդ եմ տալիս փորձել ինքնուրույն դուրս բերել մինչ դուրս բերումները կարդալը։
      </p>

      <p>
        Սկսենք

        <span id="margin_669494561666_reveal" class="equation_link">(BP1)</span>
        <span id="margin_669494561666" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_669494561666_reveal').click(function() {
            $('#margin_669494561666').toggle('slow', function() {});
          });
        </script>

        հավասարումով, որը ցույց է տալիս ելքի $\delta^L$ սխալանքը։ Հիշենք, որ ըստ սահմանման

        <a class="displaced_anchor" name="eqtn36"></a>\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial z^L_j}. \tag{36}\end{eqnarray} Կիրառելով բարդ ֆունկցիայի ածանցման կանոնը, կարող ենք վերևի մասնական ածանցյալը վերարտահայտել ելքային ակտիվացիաներից
        կախված մասնական ածանցյալների միջոցով

        <a class="displaced_anchor" name="eqtn37"></a>\begin{eqnarray} \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j}, \tag{37}\end{eqnarray} որտեղ գումարն ըստ ելքային շեւրտի $k$ նեյրոնների է։ Իհարկե, $k^{\rm
        րդ}$ նեյրոնի $a^L_k$ ակտիվացիան կախված է $j^{\rm րդ}$ նեյրոնի $z^L_j$ կշռված մուտքից միայն այն դեպքում, երբ $k = j$։ Եվ այսպիսով, $\partial a^L_k / \partial z^L_j$ վերանում է, երբ $k \neq j$։ Այսպիսով հավասարումն կարելի է պարզեցնել.

        <a class="displaced_anchor" name="eqtn38"></a>\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}. \tag{38}\end{eqnarray} Նկատենք, որ $a^L_j = \sigma(z^L_j)$, ապա աջ կողմում գտնվող երկրորդ արտահայտությունը
        կարելի է արտահայտել որպես $\sigma'(z^L_j)$ և հավասարումը կվերածվի հետևյալ տեսքի

        <a class="displaced_anchor" name="eqtn39"></a>\begin{eqnarray} \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j), \tag{39}\end{eqnarray} որը պարզապես

        <span id="margin_543019119880_reveal" class="equation_link">(BP1)</span>
        <span id="margin_543019119880" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP1" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j) \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_543019119880_reveal').click(function() {
            $('#margin_543019119880').toggle('slow', function() {});
          });
        </script>

        հավասարումն է՝ արտահայտված անդամների միջոցով։
      </p>

      <p>
        Հաջորդիվ, կապացուցենք
        <span id="margin_676464380519_reveal" class="equation_link">(BP2)</span>
        <span id="margin_676464380519" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_676464380519_reveal').click(function() {
            $('#margin_676464380519').toggle('slow', function() {});
          });
        </script>

        հավասարումը, որը $\delta^l$ սխալանքն արտահայտում է ըստ հաջորդ շերտի $\delta^{l+1}$ սխալանքի։ Այդ նպատակով, փորձենք $\delta^l_j = \partial C / \partial z^l_j$ արտահայտենք ըստ $\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$, օգտագործելով բարդ ֆունկցիայի
        դիֆերենցման կանոնը։

        <a class="displaced_anchor" name="eqtn40"></a><a class="displaced_anchor" name="eqtn41"></a><a class="displaced_anchor" name="eqtn42"></a>\begin{eqnarray} \delta^l_j & = & \frac{\partial C}{\partial z^l_j} \tag{40}\\ & = & \sum_k \frac{\partial
        C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \tag{41}\\ & = & \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k, \tag{42}\end{eqnarray} որտեղ վերջին տողում մենք երկու արտահայտությունները շրջել ենք տեղերով և աջ
        կողմը փոխարինել $\delta^{l+1}_k$ արտահայտությամբ ըստ սահմանման։ Վերջին տողի առաջին անդամը հաշվելու համար, հաշվի առնենք, որ

        <a class="displaced_anchor" name="eqtn43"></a>\begin{eqnarray} z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k. \tag{43}\end{eqnarray} Դիֆերենցելով կստանանք հետևյալ արտահայտությունը՝

        <a class="displaced_anchor" name="eqtn44"></a>\begin{eqnarray} \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j). \tag{44}\end{eqnarray} Տեղադրելով ստացվածը

        <span id="margin_863771588529_reveal" class="equation_link">(42)</span>
        <span id="margin_863771588529" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn42" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    & = & \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_863771588529_reveal').click(function() {
            $('#margin_863771588529').toggle('slow', function() {});
          });
        </script>

        հավասարման մեջ, կստանանք

        <a class="displaced_anchor" name="eqtn45"></a>\begin{eqnarray} \delta^l_j = \sum_k w^{l+1}_{kj} \delta^{l+1}_k \sigma'(z^l_j). \tag{45}\end{eqnarray} Սա պարզապես

        <span id="margin_660951274755_reveal" class="equation_link">(BP2)</span>
        <span id="margin_660951274755" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP2" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_660951274755_reveal').click(function() {
            $('#margin_660951274755').toggle('slow', function() {});
          });
        </script>

        հավասարումն է գրված էլեմենտ առ էլեմենտ տեսքով։
      </p>

      <p>
        Վերջին երկու հավասարումները, որ կապացուցենք, դրանք

        <span id="margin_869387373993_reveal" class="equation_link">(BP3)</span>
        <span id="margin_869387373993" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP3" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =
  \delta^l_j \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_869387373993_reveal').click(function() {
            $('#margin_869387373993').toggle('slow', function() {});
          });
        </script>

        և

        <span id="margin_237094968075_reveal" class="equation_link">(BP4)</span>
        <span id="margin_237094968075" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtnBP4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_237094968075_reveal').click(function() {
            $('#margin_237094968075').toggle('slow', function() {});
          });
        </script>

        հավասարումներն են, որոնք նույնպես հետևում են բարդ ֆունկցիայի դիֆերենցիալի կանոնից՝ շատ նման վերևում բերված ապացույցներին։ Դա թողնում են ընթերցողին որպես վարժություն.
      </p>

      <p>
        <h4>
    <a name="exercise_835949"></a>
    <a href="#exercise_835949">Վարժություն</a>
  </h4>
        <ul>
          <li>
            Ապացուցել
            <span id="margin_501064179739_reveal" class="equation_link">(BP3)</span>
            <span id="margin_501064179739" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP3" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =
          \delta^l_j \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_501064179739_reveal').click(function() {
                $('#margin_501064179739').toggle('slow', function() {});
              });
            </script>

            և

            <span id="margin_840492722163_reveal" class="equation_link">(BP4)</span>
            <span id="margin_840492722163" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtnBP4" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
          \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j \nonumber\end{eqnarray}</a></span>
            <script>
              $('#margin_840492722163_reveal').click(function() {
                $('#margin_840492722163').toggle('slow', function() {});
              });
            </script>

            հավասարումները։
        </ul>
      </p>

      <p>
        Այսքանով ավարտում ենք հետադարձ տարածման հիմնական հավասարումների ապացույցը։ Ապացույցը կարող է դժվար թվալ։ Սակայն այն պարզապես բարդ ֆունկցիաների դիֆերենցման կանոնի հմտորեն կիրառման հետևանք է։ Այլ կերպ ասած, կարող ենք պատկերացնել, որ հետադարձ տարածումը դա
        գնային ֆունկցիայի գրադիենտի հաշվման եղանակ է, որը հիմնված է շատ փոփոխականի բարդ ֆունկցիաների դիֆերենցման կանոնի սիստեմատիկ օգտագործման վրա։ Դա, ըստ էության նկարագրում է հետադարձ տարածման հիմնական գաղափարը, իսկ մնացածն ուղղակի մանրամասներ են։
      </p>

      <p>
        <h3>
    <a name="the_backpropagation_algorithm"></a>
    <a href="#the_backpropagation_algorithm">Հետադարձ տարածման ալգորիթմը</a>
  </h3>
      </p>

      <p>
        Հետադարձ տարածման հավասարումները ցույց են տալիս, թե ինչպես կարելի է հաշվել գնային ֆունկցիայի գրադիենտը։ Արտահայտենք դա ալգորիթմի տեսքով.

        <ol>
          <li>
            <strong>Մուտք $x$:</strong> $a^{1}$ ակտիվացիայի շերտը սկզբնավորենք մուտքային շերտով։
      </p>

      <p>
        <li>
          <strong>Առաջաբերում (Feedforward):</strong> Յուրաքանչյուր $l = 2, 3, \ldots, L$ շերտերի համար հաշվել $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \sigma(z^{l})$.
      </p>

      <p>
        <li>
          <strong>Ելքային սխալանք $\delta^L$:</strong> Հաշվել $\delta^{L} = \nabla_a C \odot \sigma'(z^L)$ վեկտորը։
      </p>

      <p>
        <li>
          <strong>Սխալանքի հետադարձ տարածումը (Backpropagate)։</strong> Յուրաքանչյուր $l = L-1, L-2, \ldots, 2$ շերտի համար հաշվել $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$.
      </p>

      <p>
        <li>
          <strong>Ելք:</strong> Գնային ֆունկցիայի գրադիենտը տրվում է $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ և $\frac{\partial C}{\partial b^l_j} = \delta^l_j$ հավասարումներով։
          </ol>
      </p>

      <p>
        Դիտարկելով ալգորիթմը, կարելի է տեսնել, թե ինչու է այն կոչվում <em>հատադարձ</em> տարածում։ Սխալանքի վեկտորները հետադարձ հաշվարկվում են՝ սկսելով վերջին շերտից։ Հետադարձ գործողությունը հետվանք է այն բանի, որ գնային ֆունկցիան կախված է ցանցի ելքային
        արժեքներից։ Որպեսզի հասկանանք գնի փոփոխությունը կախված կշիռներից և շեղումներից, պետք է կրկնողաբար կիրառենք բարդ ֆունկցիայի դիֆերենցման կանոնը, հետ վերադառնալով շերտ առ շերտ, որպեսզի ստանանք համապատասխան արտահայտությունները։
      </p>

      <p>
        <h4>
    <a name="exercises_675621"></a>
    <a href="#exercises_675621">Վարժություններ</a>
  </h4>
        <ul>
          <li>
            <strong>
        Հետադարձ տարածում միակ փոփոխության ենտարկված նեյրոնով։
      </strong> Ենթադրենք, որ տրված է առաջաբեր նեյրոնային ցանց, որտեղ գոյություն ունի միայն մեկ նեյրոն, որը փոփոխության է ենտարկված, այնպես, որ նեյրոնի ելքային արժեքը տրված է $f(\sum_j w_j x_j + b)$ արտահայտությամբ, որտեղ $f$ որևէ ֆունկցիա
            է բացի սիգմոիդից։ Ինչպես պետք է փոփոխության ենթարկենք հետադարձ տարածման ալգորիթմն այս դեպքում։
      </p>

      <p>
        <li>
          <strong>Գծային նեյրոններով հետադարձ տարածում։</strong> Ենթադրենք, որ $\sigma$ ոչ գծային ֆունկցիան փոխարինել ենք $\sigma(z) = z$ գծային ֆուննկցիայով ամբողջ ցանցի համար։ Վերաձևակերպեք հետադարձ տարածման ալգորիմթմն այս դեպքում։
          </ul>
      </p>

      <p>
        Ինչպես վերևում նկարագրված է, հետադարձ տարածման ալգորիթմը հաշվում է գնային ֆունկցիայի գրադիենտը տրված $C = C_x$ մարզման օրինակի համար։ Հաճախ հետադարձ տարածումը միավորվում է այնպիսի ուսուցման ալգորիթմի հետ, ինչպիսին է ստոկաստիկ գրադիենտային վայրէջքը, որտեղ
        գրադիենտը հաշվարկվում է բազմաթիվ օրինակների հիման վրա։ Տրված $m$ մարզման օրինակների մինի-փաթեթի համար, հետևյալ ալգորիթմը կիրառում է գրադիենտային վայրէջքի ուսուցման քայլը՝ հիմնված այդ մինի-փաթեթի վրա։
        <ol>
          <li>
            <strong>Մուտքագրեք մարզման օրինակների բազմությունը։</strong>
      </p>

      <p>
        <li>
          <strong>Յուրաքանչյուր $x$ մարզման օրինակի համար.</strong> Սկզբնավորենք համապատասխան $a^{x,1}$ մուտքային ակտիվացիան և կատարենք հետևյալ քայլերը.
      </p>

      <p>
        <ul>
          <li>
            <strong>Առաջաբերում (Feedforward):</strong> Յուրաքանչյուր $l = 2, 3, \ldots, L$ շերտի համար հաշվենք $z^{x,l} = w^l a^{x,l-1}+b^l$ և $a^{x,l} = \sigma(z^{x,l})$ արտահայտությունների արժեքները։
      </p>

      <p>
        <li>
          <strong>Ելքային սխալանքը՝ $\delta^{x,L}$:</strong> Վեկտորի հաշվումը. $\delta^{x,L} = \nabla_a C_x \odot \sigma'(z^{x,L})$.
      </p>

      <p>
        <li>
          <strong>Սխալանքի հետադարձ տարածումը:</strong> Յուրաքանչյուր $l = L-1, L-2, \ldots, 2$ հաշվել $\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1}) \odot \sigma'(z^{x,l})$.
          </ul>
      </p>

      <p>
        <li>
          <strong>Գրադիենտային վայրէջք:</strong> Յուրաքանչյուր $l = L, L-1, \ldots, 2$ թարմացնել կշիռները ըստ հետևյալ կանոնի՝ $w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T$, և շեղումներն ըստ $b^l \rightarrow b^l-\frac{\eta}{m}
          \sum_x \delta^{x,l}$ կանոնի.
      </p>

      <p>
        </ol>
        Իհարկե, պրակտիկորեն ստոկաստիկ գրադիենտային վայրէջք իրականացնելու համար նաև պետք է արտաքին ցիկլ, որը կգեներացնի մարզման օրինակների մինի-փաթեթները և արտաքին ցիկլ, որը կիրականացնի մարզման տարբեր դարաշրջանները։ Դրանք բաց են թողնված պարզության համար։
      </p>

      <p></p>

      <p>
        <h3>
    <a name="the_code_for_backpropagation"></a>
    <a href="#the_code_for_backpropagation">Հետադարձ տարածման իրականացման կոդը</a>
  </h3>
      </p>

      <p>
        Ըմբռնելով հետադարձ տարածման աբստրակտ տարբերակը կարելի է առաջ անցնել և դիտարկել նախկին գլխում օգտագործված հետադարձ տարածման իրականացման կոդը։ Հիշենք
        <a href="chap1.html#implementing_our_network_to_classify_digits">
    նախորդ գլխից,
  </a> որ կոդը զետեղված է <tt>Network</tt> դասի <tt>update_mini_batch</tt> և
        <tt>backprop</tt> մեթոդներում։ Այս մեթոդների կոդը վերևում նկարագրված ալգորիթմների ուղիղ թարգմանությունն է։ Մասնավորապես <tt>update_mini_batch</tt> մեթոդը թարմացնում է <tt>Network</tt>-ի կշիռներն ու շեղումները՝ հաշվելով ընթացիկ մարզման օրինակների
        <tt>mini_batch</tt>-ի գրադիենտը.
        <div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="o">...</span>
      <span class="k">def</span> <span class="nf">update_mini_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
          <span class="sd">&quot;&quot;&quot;Update the network&#39;s weights and biases by applying</span>
  <span class="sd">        gradient descent using backpropagation to a single mini batch.</span>
  <span class="sd">        The &quot;mini_batch&quot; is a list of tuples &quot;(x, y)&quot;, and &quot;eta&quot;</span>
  <span class="sd">        is the learning rate.&quot;&quot;&quot;</span>
          <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
          <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>
          <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
              <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
              <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">nb</span><span class="o">+</span><span class="n">dnb</span> <span class="k">for</span> <span class="n">nb</span><span class="p">,</span> <span class="n">dnb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_b</span><span class="p">)]</span>
              <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">nw</span><span class="o">+</span><span class="n">dnw</span> <span class="k">for</span> <span class="n">nw</span><span class="p">,</span> <span class="n">dnw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_w</span><span class="p">,</span> <span class="n">delta_nabla_w</span><span class="p">)]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nw</span>
                          <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">nw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)]</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nb</span>
                         <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">nabla_b</span><span class="p">)]</span>
  </pre></div>

        Գործի մեծ մասը կատարվում է մինչև <tt>delta_nabla_b, delta_nabla_w = self.backprop(x, y)</tt> տողը, որն օգտագործում է <tt>backprop</tt> մեթոդը $\partial C_x / \partial b^l_j$ and $\partial C_x / \partial w^l_{jk}$ դիֆերենցիալները հաշվելու համար։
        <tt>backprop</tt> մեթոդը սերտորեն հետևում է նախորդ բաժնում նկարագրված ալգորիթմը։ Միայն մեկ փոքր փոփոխություն՝ շերտերի ինդեքսավորման փոքր-ինչ տարբեր մոտեցում է օգտագործված։ Այս փոփոխությունը Python ծրագրի որոշ հնարավորություններից օգտվելու նպատակով
        է, այն է, որպեսզի օգտագործի ցուցակի բացասական ինդեքսներ ցուցակի վերջից դեպի սկիզբ հաշվելու համար, օրինակ
        <tt>l[-3]</tt>-ը վերջից երրորդ անդամն է <tt>l</tt> ցուցակում։ Ներքևում ներկայացված է
        <tt>backprop</tt> կոդը, որոշ օգնական (helper) ֆունկցիաների հետ միասին, որոնք օգտագործված են $\sigma$ ֆունկցիայի հաշվման, $\sigma'$ ածանցյալի հաշվման և գնի ֆունկցիայի ածանցյալի հաշվման համար։ Ներքևում զետեղված կոդը կարծում եմ հասկանալի կլինի, սակայն
        խնդիրների դեպքում խորհուրդ եմ տալիս վերադառնալ
        <a href="chap1.html#implementing_our_network_to_classify_digits">
    բնօրինակին և կոդի ամբողջական ներկայացմանը։
  </a>.

        <div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="o">...</span>
     <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="sd">&quot;&quot;&quot;Return a tuple &quot;(nabla_b, nabla_w)&quot; representing the</span>
  <span class="sd">        gradient for the cost function C_x.  &quot;nabla_b&quot; and</span>
  <span class="sd">        &quot;nabla_w&quot; are layer-by-layer lists of numpy arrays, similar</span>
  <span class="sd">        to &quot;self.biases&quot; and &quot;self.weights&quot;.&quot;&quot;&quot;</span>
          <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
          <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>
          <span class="c1"># feedforward</span>
          <span class="n">activation</span> <span class="o">=</span> <span class="n">x</span>
          <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="c1"># list to store all the activations, layer by layer</span>
          <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list to store all the z vectors, layer by layer</span>
          <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
              <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
              <span class="n">zs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
              <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
              <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
          <span class="c1"># backward pass</span>
          <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
              <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
          <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
          <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
          <span class="c1"># Note that the variable l in the loop below is used a little</span>
          <span class="c1"># differently to the notation in Chapter 2 of the book.  Here,</span>
          <span class="c1"># l = 1 means the last layer of neurons, l = 2 is the</span>
          <span class="c1"># second-last layer, and so on.  It&#39;s a renumbering of the</span>
          <span class="c1"># scheme in the book, used here to take advantage of the fact</span>
          <span class="c1"># that Python can use negative indices in lists.</span>
          <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
              <span class="n">z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
              <span class="n">sp</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
              <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sp</span>
              <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
              <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
          <span class="k">return</span> <span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)</span>

  <span class="o">...</span>

      <span class="k">def</span> <span class="nf">cost_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_activations</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="sd">&quot;&quot;&quot;Return the vector of partial derivatives \partial C_x /</span>
  <span class="sd">        \partial a for the output activations.&quot;&quot;&quot;</span>
          <span class="k">return</span> <span class="p">(</span><span class="n">output_activations</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
  </pre></div>
      </p>

      <p>
        <a id="backprop_over_minibatch"></a>
        <h4>
    <a name="problem_269962"></a>
    <a href="#problem_269962">Խնդիր</a>
  </h4>
        <ul>
          <li>
            <strong>
        Լրիվ մատրիցային մոտեցում մինի-փաթեթով հետադարձ տարածման համար։
      </strong> Մեր ստոկաստիկ գրադիենտային վայրէջքը անցնում է մարզման օրինակների վրայով մինի-փաթեթում։ Հնարավոր է կատարել այնպիսի փոփոխություն, որի արդյունքում հետադարձ տարածման ալգորիթմը միաժամանակ կհաշվի բոլոր օրինակների գրադիենտները մինի-փաթեթում։
            Գաղափարը կայանում է նրանում, որ մեկ $x$ մուտքային վեկտորից սկսելու փոխարեն, կարող ենք սկսել $X = [x_1 x_2 \ldots x_m]$ մատրիցով, որի սյունակները մինի-փաթեթի վեկտորներն են։ Առաջ տարածումը (forward-propagate) կատարվում է կշիռների մատրիցով բազմապատկման,
            շեղումների մատրիցով գումարման և սիգմոիդ ֆունկցիայի կիրառմամբ մատրիցի բոլոր էլեմենտներին։ Հետադարձ տարածումը կատարվում է նմանատիպ մոտեցումներով։ Կառուցեք հետադարձ տարածման պսեվդոկոդը համաձայն այս մոտեցման։ Փոփոխեք <tt>network.py</tt> այնպես,
            որ այն օգտագործի մատրիցային մոտեցումը։ Այս մոտեցման առավելությունը կայանում է նրանում, որ այն օգտագործում է ժամանակակից գծային հանրահաշվի գրադարանների լայն հնարավորությունները։ Արդյունքում այն էապես ավելի արագ կլինի, քան մինի-փաթեթի վրա կառուցված
            ցիկլը։ (Իմ լափթոփի վրա, օրինակ, մոտ երկու անգամ ավելի արագ է, երբ գործարկում եմ MNIST դասակարգման խնդիրների վրա, նկարագրված նախորդ գլխում։) Պրակտիկորեն բոլոր լուրջ գրադարանները, որոնք իրականացնում են հետադարձ տարածում, օգտագործում այս են մատրիցային
            մոտեցման ինչ-որ տարբերակ։
        </ul>
      </p>

      <p>
        <h3>
    <a name="in_what_sense_is_backpropagation_a_fast_algorithm"></a>
    <a href="#in_what_sense_is_backpropagation_a_fast_algorithm">
      Ի՞նչ իմաստով է հետադարձ տարածումն արագագործ ալգորիթմ
    </a>
  </h3>
      </p>

      <p>
        Ի՞նչ իմաստով է հետադարձ տարածումն արագագործ ալգորիթմ։ Այս հարցին պատասխանելու համար դիտարկենք գրադիենտի հաշվման այլ մոտեցում։ Պատկերացրեք նեյրոնային ցանցերի հետազոտությունների սկզբնական շրջանում ենք՝ 1950-ականներ կամ 1960-ականներ։ Դուք առաջին մարդն եք
        աշխարհում, որ մտածում է գրադիենտային վայրէջքն օգտագործել ուսուցման համար։ Սակայն որպեսզի ալգորիթմն աշխատի, անհրաժեշտ է մի եղանակ, որով կարելի է հաշվել գնային ֆունկցիայի գրադիենտը։ Հետ նայելով ձեր մաթեմատիկական անալիզի գիտելիքներին, փորձում եք
        տեսնել արդյոք բարդ ֆունկցիայի դիֆերենցման կանոնով կարելի է փորձել լուծել այս խնդիրը։ Սակայն որոշ ժամանակ փորձելուց հետո գաղափարը բարդ է թվում և դուք հուսահատվում եք։ Հետևաբար փորձում եք այլ մոտեցում փնտրել և դիտարկել գնի ֆունկցիան կախված միայն
        կշիռներից $C = C(w)$ (շեղումներին դեռ կվերադառնանք)։ Այնուհետև համարակալում եք կշիռները՝ $w_1, w_2, \ldots$ և հաշվում $\partial C / \partial w_j$ մասնական ածանցյալները տրված $w_j$ կշռի համար։ Կարելի է օգտագործել հետևյալ մոտարկումը՝

        <a class="displaced_anchor" name="eqtn46"></a>\begin{eqnarray} \frac{\partial C}{\partial w_{j}} \approx \frac{C(w+\epsilon e_j)-C(w)}{\epsilon}, \tag{46}\end{eqnarray} որտեղ $\epsilon > 0$ փոքր դրական թիվ է և $e_j$ միավոր վեկտորն է $j^{\rm րդ}$
        ուղղության վրա։ Այսպիսով, կարող ենք գնահատել $\partial C / \partial w_j$ մասնական ածանցյալները հաշվելով $C$ գինը երկու տարբեր (իրար մոտ) $w_j$ արժեքների համար և կիրառել

        <span id="margin_674486079288_reveal" class="equation_link">(46)</span>
        <span id="margin_674486079288" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn46" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial
    C}{\partial w_{j}} \approx \frac{C(w+\epsilon
    e_j)-C(w)}{\epsilon} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_674486079288_reveal').click(function() {
            $('#margin_674486079288').toggle('slow', function() {});
          });
        </script>

        հավասարումը։ Նույն ձևով կարող ենք հաշվել նաև $\partial C / \partial b$ շեղումների դեպքում։
      </p>

      <p>
        Այս մոտեցումը խոստումնալից է թվում։ Այն հիմնված է համեմատաբար պարզ կոնցեպտների վրա և բավականին դյուրին է իրականացնելը՝ օգտագործելով մի քանի տող կոդ։ Իրոք, այն ավելի խոստումնալից է թվում քան բարդ ֆունկցիայի ածանցյալն օգտագործելով գրադիենտ հաշվելը։
      </p>

      <p>
        Դժբախտաբար, մինչդեռ այս մոտեցումը խոստումնալից է թվում, իրականացումը բավականին դանդաղագործ է։ Որպեսզի հասկանանք պատճառները, ենթադրենք, որ մեր ցանցում ունենք միլիոն կշիռներ։ Յուրաքանչյուր տարբեր $w_j$ կշռի համար հաշվենք $C(w+\epsilon e_j)$, որպեսզի հաշվենք
        $\partial C / \partial w_j$ մասնական ածանցյալը։ Այսեղից հետևում է, որպեսզի հաշվենք գրադիենտը, պետք է գնային ֆունկցիայի արժեքը միլիոն անգամներ հաշվենք, ինչի հետևանքով միլիոն անգամ պետք է առաջ շարժվենք ցանցով։ Դա չի ներառում $C(w)$ արժեքի հաշվումը։
      </p>

      <p>
        Հետադարձ տարածման գեղեցկությունը կայանում է նրանում, որ այն միաժամանակ հաշվում է <em>բոլոր</em> $\partial C / \partial w_j$ մասնական ածանցյալները՝ օգտագործելով միայն մեկ առաջ տարածում և մեկ հետադարձ տարածում ցանցով։ Հետադարձ առաջխաղացման հաշվարկային
        արագությունը (computational cost) համարյա նույնն է, ինչ առաջ տարածման*

        <span class="marginnote">
    *Դա հնարավոր է, սակայն ճշգրիտ արտահայտությունն անելու համար որոշակի
    անալիզ և հետազոտություն է հարկավոր։ Ինչու՞ է հնարավոր, քանզի առաջ տարածման
    դեպքում հաշվարկային գինը հիմնականում բաղկացած է կշիռների մատրիցով բազմապատկումներից,
    իսկ հետադարձ տարածման դեպքում տրանսպոնացված կշիռների մատրիցներով բազմապատկումից։
    Հետևաբար այդ գործողություններն ունեն նմանատիպ հաշվարկային բարդություններ։
  </span>։ Հետևաբար հետադարձ տարածման ընդհանուր հաշվարկաւյին գինը նույնն է, ինչ ցանցում երկու առաջաբեր գործողություն կատարելիս։ Համեմատեք դա միլիոնից ավել առաջաբեր գործողությունների հետ, որ անհրաժեշտ կլիներ, եթե մենք որդեգրեինք ներքևում նշված
        հավասարման հիման վրա կառուցված մեթոդը։

        <span id="margin_884034767764_reveal" class="equation_link">(46)</span>
        <span id="margin_884034767764" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn46" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial
    C}{\partial w_{j}} \approx \frac{C(w+\epsilon
    e_j)-C(w)}{\epsilon} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_884034767764_reveal').click(function() {
            $('#margin_884034767764').toggle('slow', function() {});
          });
        </script>

        Եվ այսպիսով, չնայած նրան, որ հետադարձ տարածումն ունի բավականին բարդ տեսք, քան

        <span id="margin_806797723031_reveal" class="equation_link">(46)</span>
        <span id="margin_806797723031" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn46" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}  \frac{\partial
    C}{\partial w_{j}} \approx \frac{C(w+\epsilon
    e_j)-C(w)}{\epsilon} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_806797723031_reveal').click(function() {
            $('#margin_806797723031').toggle('slow', function() {});
          });
        </script>

        մոտեցումը, այն անհամեմատ ավելի արագագործ է։
      </p>

      <p>
        Այս «արագացումն» առաջին անգամ գնահատվել է 1986 թվականին և այն էապես ընդլայնել է խնդիրների ցանկը, որոնք կարելի է լուծել նեյրոնային ցանցերի միջոցով։ Արդյունքում, այն ստեղծեց մարդկանց ներհոսք դեպի նեյրոնային ցանցեր։ Իհարկե, հետադարձ տարածումը «դարման չէ
        բոլոր վերքերի»։ Անգամ 1980-ականների վերջերին սահմանափակումներ ծառացան գիտնականների առջև, հատկապես երբ այն փորձվեց օգտագործել խորը ցանցերի ուսուցման մեջ (բազմաթիվ շերտերով ցանցերի)։ Հետագայում կտեսնենք, թե ինչպես ժամանակակից համակարգիչները և որոշ
        խելացի գաղափարներ հնարավոր են դարձնում հետադարձ տարածման օգտագործումը այդպիսի ցանցերի մարզման համար։
      </p>

      <p>
        <h3>
    <a name="backpropagation_the_big_picture"></a>
    <a href="#backpropagation_the_big_picture">Հետադարձ տարածում. ամբողջական պատկերը</a>
  </h3>
      </p>

      <p>
        Այսպիսով, գոյություն ունեն երկու առեղծվածներ կապված հետադարձ տարածման հետ։ Առաջին. ի՞նչ է ըստ էության ալգորիթմն անում։ Իհարկե, մենք ցույց տվեցինք, որ սխալանքը ելքից տարածվում է դեպի ետ, սակայն արդյո՞ք կարող ենք ավելի խորանալ և ձեռք բերել ինտուցիա այն
        մասին, թե ինչ է տեղի ունենում երբ մենք այսքան մատրիցներ և վեկտորներ իրար ենք բազմապատկում։ Երկրոդ առեղծխվածն այն է, թե ինչպե՞ս կարելի է հայտնագործել հետադարձ տարածման ալգորիթմը։ Իհարկե կարելի է հետևել և հասկանալ ալգորիթմի քայլերն ու ապացույցը,
        սակայն արդյո՞ք խնդիրը հասկանում ենք այնքան խորությամբ, որ կարող էինք ալգրորիթմն ինքնուրույն հայտնագործել։ Արդյոք կա տրամաբանական շղթա, որին հետևելով կարող էինք հանգել հետադարձ տարածման ալգորիթմին։ Այս հատվածում կդիտարկենք այդ երկու առեղծվածները։
      </p>

      <p>
        Որպեսզի մեր ինտուցիան այս ալգորիթմի վերաբերյալ բարելավենք, ենթադերնք, որ ցանցի որևիցե $w^l_{jk}$ կշռի նկատմամբ կիրառել ենք $\Delta w^l_{jk}$ փոփոխությունը.

        <center>
          <img src="images/tikz22.png" />
        </center>

        Այս փոփոխությունը կհանգեցնի համապատասխան փոփոխություն այդ նեյրոնին համապատասխանող ելքային ակտիվացիայում։

        <center>
          <img src="images/tikz23.png" />
        </center>

        Որն իր հերթին կհանգեցնի հաջորդ շերտի <em>բոլոր</em> ակտիվացիաների փոփոխության։

        <center>
          <img src="images/tikz24.png" />
        </center>

        Այդ փոփոխություննեն իրենց հերթին կառաջացնեն փոփոխութւուններ հաջորդ շերտում, այնուհետև հաջորդ և այդպես շարունակ մինչև վերջնական շերտը հանգեցնելով փոփոխության գնային ֆունկցիայում։

        <center>
          <img src="images/tikz25.png" />
        </center>

        Գնային ֆունկցիայի $\Delta C$ փոփոխությունը կշռի $\Delta w^l_{jk}$ փոփոխության հետ կապված է հետևյալ հավասարմամբ.

        <a class="displaced_anchor" name="eqtn47"></a>\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk}. \tag{47}\end{eqnarray} Սա ցույց է տալիս, որ $\frac{\partial C}{\partial w^l_{jk}}$ հաշվարկելու հնարավոր տարբերակներից
        մեկն այն է, որ հետևենք, թե ինչպես է $w^l_{jk}$ տարծվում և փոքրիկ փոփոխություն առաջացնում $C$ գնային ֆունկցիայում։ Եթե դա ուշադիր կերպով իրականացնենք, հաշվելով բոլոր միջանկյալ արժեքները, ապա կարող ենք հաշվել $\partial C / \partial w^l_{jk}$ արեքը…
      </p>

      <p>
        Փորձենք դա իրականացնել։ $\Delta w^l_{jk}$ փոփոխությունն առաջացնում է $\Delta a^{l}_j$ փոքրիկ փոփոխությունը $l^{\rm րդ}$ շերտի $j^{\rm րդ}$ ակտիվացիայում։ Այդ փոփոխությունը կարելի է նկարագրել հետևյալ կերպ։

        <a class="displaced_anchor" name="eqtn48"></a>\begin{eqnarray} \Delta a^l_j \approx \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}. \tag{48}\end{eqnarray} Փոփոխությունը $\Delta a^l_{j}$ ակտիվացիայում կառաջացնի փոփոխություններ հաջորդ
        <em>բոլոր</em> շերտերի ակտիվացիաներում, օրինակ, $(l+1)^{\rm րդ}$ շերտում։ Դիտարկենք, թե ինչպես է փոփոխվում այդ ակտիվացիաներից որևիցէ մեկը, օրինակ՝ $a^{l+1}_q$։

        <center>
          <img src="images/tikz26.png" />
        </center>

        Ըստ էության, հետևյալ փոփոխությունը տեղի կունենա.

        <a class="displaced_anchor" name="eqtn49"></a>\begin{eqnarray} \Delta a^{l+1}_q \approx \frac{\partial a^{l+1}_q}{\partial a^l_j} \Delta a^l_j. \tag{49}\end{eqnarray} Կատարելով փոխարինում
        <span id="margin_531795813347_reveal" class="equation_link">(48)</span>
        <span id="margin_531795813347" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn48" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \Delta a^l_j \approx \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_531795813347_reveal').click(function() {
            $('#margin_531795813347').toggle('slow', function() {});
          });
        </script>, հավասարումով, կստանանք.

        <a class="displaced_anchor" name="eqtn50"></a>\begin{eqnarray} \Delta a^{l+1}_q \approx \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}. \tag{50}\end{eqnarray} Իհարկե, $\Delta a^{l+1}_q$ ակտիվացիայի
        փոփոխությունն իր հերթին կհանգեցնի փոփոխությունների հաջորդ շերտերում։ Կարող ենք պատկերացնել ցանցում այդպիսի փոփոխությունների ուղի՝ $w^l_{jk}$-ից դեպի $C$ գնային ֆունկցիա, որտեղ յուրաքանչյուր փոփոխություն ակտիվացիայում հանգեցնում է հաջորդ շերտի
        ակտիվացիաների փոփոխության, վերջապես շղթայաբար հասնելով գնային ֆունկցիայի փոփոխության ելքում։ Եթե այդ ուղին անցնում է $a^l_j, a^{l+1}_q, \ldots, a^{L-1}_n, a^L_m$ ակտիվացիաներով, ապա գնային ֆունկցիայի փոփոխությունը կարելի է նկարագրել հետևյալ կերպ.


        <a class="displaced_anchor" name="eqtn51"></a>\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial
        a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}, \tag{51}\end{eqnarray} որտեղ յուրաքանչյուր փոփոխված նեյրոնի համար կիրառել ենք $\partial a / \partial a$ տիպի փոփոխություն նկարագրող արտահայտությունը՝ վերջում կիրառելով $\partial
        C/\partial a^L_m$ գնային ֆունկցիայի փոփոխությունը կախված ելքային նեյրոնից։ Այսպիսով, $C$-ի փոփոխությունը կարելի է ներկայացնել այս կերպ՝ հիմնված ակտիվացիաների շղթայական փոփոխությունների վրա։ Իհարկե, գոյություն ունեն բազմաթիվ ուղիներ, որով $w^l_{jk}$
        փոփոխությունը կարող է տարածվել՝ ազդելով գնային ֆունկցիայի վրա, և մենք դիտարկեցինք այդ ուղիներից միայն մեկը։ $C$ գնային ֆունկցիայի ամբողջական փոփոխությունը հաշվելու համար կարծես թե պետք է գումարել փոփոխությունները տրված կշռից դեպի գնային ֆունկցիա
        բոլոր հնարավոր ուղիներով։ Օրինակ.

        <a class="displaced_anchor" name="eqtn52"></a>\begin{eqnarray} \Delta C \approx \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial
        a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}, \tag{52}\end{eqnarray} որտեղ գումարն ըստ բոլոր հնարավոր միջանկյալ նեյրոնների է։ Համեմատելով

        <span id="margin_103206561462_reveal" class="equation_link">(47)</span>
        <span id="margin_103206561462" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn47" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_103206561462_reveal').click(function() {
            $('#margin_103206561462').toggle('slow', function() {});
          });
        </script>

        հավասարման հետ, կտեսնենք, որ

        <a class="displaced_anchor" name="eqtn53"></a>\begin{eqnarray} \frac{\partial C}{\partial w^l_{jk}} = \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p}
        \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}}. \tag{53}\end{eqnarray} Այժմ

        <span id="margin_963684443937_reveal" class="equation_link">(53)</span>
        <span id="margin_963684443937" class="marginequation" style="display: none;"><a href="chap2.html#eqtn53" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \frac{\partial C}{\partial w^l_{jk}} = \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m}
    \frac{\partial a^L_m}{\partial a^{L-1}_n}
    \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots
    \frac{\partial a^{l+1}_q}{\partial a^l_j}
    \frac{\partial a^l_j}{\partial w^l_{jk}} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_963684443937_reveal').click(function() {
            $('#margin_963684443937').toggle('slow', function() {});
          });
        </script>

        հավասարումը բարդ տեսք ունի։ Սակայն այն ունի պարզ ինտուիտիվ մեկնաբանություն։ Մենք հաշվում ենք $C$ փոփոխության գործակիցը կախված ցանցի կշռից։ However, it has a nice intuitive interpretation. We're computing the rate of change of $C$ with respect to a weight
        in the network. What the equation tells us is that every edge between two neurons in the network is associated with a rate factor which is just the partial derivative of one neuron's activation with respect to the other neuron's activation. The
        edge from the first weight to the first neuron has a rate factor $\partial a^{l}_j / \partial w^l_{jk}$. The rate factor for a path is just the product of the rate factors along the path. And the total rate of change $\partial C / \partial w^l_{jk}$
        is just the sum of the rate factors of all paths from the initial weight to the final cost. This procedure is illustrated here, for a single path:

        <center>
          <img src="images/tikz27.png" />
        </center>
      </p>

      <p>
        What I've been providing up to now is a heuristic argument, a way of thinking about what's going on when you perturb a weight in a network. Let me sketch out a line of thinking you could use to further develop this argument. First, you could derive explicit
        expressions for all the individual partial derivatives in Equation

        <span id="margin_602209892794_reveal" class="equation_link">(53)</span>
        <span id="margin_602209892794" class="marginequation" style="display: none;">
    <a href="chap2.html#eqtn53" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
    \frac{\partial C}{\partial w^l_{jk}} = \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m}
    \frac{\partial a^L_m}{\partial a^{L-1}_n}
    \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots
    \frac{\partial a^{l+1}_q}{\partial a^l_j}
    \frac{\partial a^l_j}{\partial w^l_{jk}} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_602209892794_reveal').click(function() {
            $('#margin_602209892794').toggle('slow', function() {});
          });
        </script>

        . That's easy to do with a bit of calculus. Having done that, you could then try to figure out how to write all the sums over indices as matrix multiplications. This turns out to be tedious, and requires some persistence, but not extraordinary insight.
        After doing all this, and then simplifying as much as possible, what you discover is that you end up with exactly the backpropagation algorithm! And so you can think of the backpropagation algorithm as providing a way of computing the sum over
        the rate factor for all these paths. Or, to put it slightly differently, the backpropagation algorithm is a clever way of keeping track of small perturbations to the weights (and biases) as they propagate through the network, reach the output,
        and then affect the cost.
      </p>

      <p>
        Now, I'm not going to work through all this here. It's messy and requires considerable care to work through all the details. If you're up for a challenge, you may enjoy attempting it. And even if not, I hope this line of thinking gives you some insight
        into what backpropagation is accomplishing.
      </p>

      <p>
        What about the other mystery - how backpropagation could have been discovered in the first place? In fact, if you follow the approach I just sketched you will discover a proof of backpropagation. Unfortunately, the proof is quite a bit longer and more
        complicated than the one I described earlier in this chapter. So how was that short (but more mysterious) proof discovered? What you find when you write out all the details of the long proof is that, after the fact, there are several obvious simplifications
        staring you in the face. You make those simplifications, get a shorter proof, and write that out. And then several more obvious simplifications jump out at you. So you repeat again. The result after a few iterations is the proof we saw earlier*

        <span class="marginnote">
    *There is one clever step required.  In
      Equation
      <span id="margin_817509150234_reveal" class="equation_link">(53)</span>
        <span id="margin_817509150234" class="marginequation" style="display: none;">
        <a href="chap2.html#eqtn53" style="padding-bottom: 5px;" onMouseOver="this.style.borderBottom='1px solid #2A6EA6';" onMouseOut="this.style.borderBottom='0px';">\begin{eqnarray}
        \frac{\partial C}{\partial w^l_{jk}} = \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m}
        \frac{\partial a^L_m}{\partial a^{L-1}_n}
        \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots
        \frac{\partial a^{l+1}_q}{\partial a^l_j}
        \frac{\partial a^l_j}{\partial w^l_{jk}} \nonumber\end{eqnarray}</a></span>
        <script>
          $('#margin_817509150234_reveal').click(function() {
            $('#margin_817509150234').toggle('slow', function() {});
          });
        </script>

        the intermediate variables are activations like $a_q^{l+1}$. The clever idea is to switch to using weighted inputs, like $z^{l+1}_q$, as the intermediate variables. If you don't have this idea, and instead continue using the activations $a^{l+1}_q$, the
        proof you obtain turns out to be slightly more complex than the proof given earlier in the chapter.
        </span>
        - short, but somewhat obscure, because all the signposts to its construction have been removed! I am, of course, asking you to trust me on this, but there really is no great mystery to the origin of the earlier proof. It's just a lot of hard work simplifying
        the proof I've sketched in this section.
      </p>

      <p><br/><br/><br/></p>
      <p>
  </div>
  <div class="footer"> <span class="left_footer"> In academic work,
please cite this book as: Michael A. Nielsen, "Neural Networks and
Deep Learning", Determination Press, 2015

<br/>
<br/>

This work is licensed under a <a rel="license"
href="http://creativecommons.org/licenses/by-nc/3.0/deed.en_GB"
style="color: #eee;">Creative Commons Attribution-NonCommercial 3.0
Unported License</a>.  This means you're free to copy, share, and
build on this book, but not to sell it.  If you're interested in
commercial use, please <a
href="mailto:mn@michaelnielsen.org">contact me</a>.
</span>
    <span class="right_footer">
Last update: Thu Jan 19 06:09:48 2017
<br/>
<br/>
<br/>
<a rel="license" href="http://creativecommons.org/licenses/by-nc/3.0/deed.en_GB"><img alt="Creative Commons Licence" style="border-width:0" src="http://i.creativecommons.org/l/by-nc/3.0/88x31.png" /></a>
</span>
  </div>
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-44208967-1', 'neuralnetworksanddeeplearning.com');
    ga('send', 'pageview');
  </script>
</body>

</html>
